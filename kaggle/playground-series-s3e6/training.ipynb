{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data, EDA, Feature Engineering, and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: (32730, 18)\n",
      "Test Size: (15154, 17)\n",
      "Num Features: 17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((32730, 18), (15154, 17))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "TARGET = 'price'\n",
    "COMPETITION = 'playground-series-s3e6'\n",
    "\n",
    "def load_data(target = TARGET,load_original=True):\n",
    "    # Load the data, turn it into tensors\n",
    "    train_df = pd.read_csv('train.csv', index_col='id')\n",
    "    test_df = pd.read_csv('test.csv', index_col='id')\n",
    "\n",
    "    # Add generative flags\n",
    "    train_df['is_generated'] = 1\n",
    "    test_df['is_generated'] = 1\n",
    "\n",
    "    if load_original:\n",
    "        original = pd.read_csv('ParisHousing.csv')\n",
    "        original['is_generated'] = 0\n",
    "        ext_df = original\n",
    "        train_df = pd.concat([ext_df,train_df])\n",
    "        train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    print(\"Training Size:\", train_df.shape)\n",
    "    print(\"Test Size:\", test_df.shape)\n",
    "    \n",
    "    # Reset index\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    \n",
    "    FEATURES = [col for col in train_df.columns if col not in ['id', target]]\n",
    "    print(\"Num Features:\", len(FEATURES))\n",
    "\n",
    "\n",
    "    train_tensors = torch.tensor(train_df[FEATURES].values, dtype=torch.float32)\n",
    "    target_tensors = torch.tensor(train_df[target].values, dtype=torch.float32)\n",
    "    test_tensors = torch.tensor(test_df[FEATURES].values, dtype=torch.float32)\n",
    "    output_dict = {\n",
    "        \"train_df\": train_df,\n",
    "        \"test_df\": test_df,\n",
    "        \"train_tensors\": train_tensors,\n",
    "        \"target_tensors\": target_tensors,\n",
    "        \"test_tensors\": test_tensors,\n",
    "    }\n",
    "    return output_dict\n",
    "\n",
    "load_dict = load_data(load_original=True)\n",
    "\n",
    "train_df = load_dict['train_df']\n",
    "test_df = load_dict['test_df']\n",
    "\n",
    "train_df.shape,test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32730 entries, 0 to 32729\n",
      "Data columns (total 18 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   squareMeters       32730 non-null  int64  \n",
      " 1   numberOfRooms      32730 non-null  int64  \n",
      " 2   hasYard            32730 non-null  int64  \n",
      " 3   hasPool            32730 non-null  int64  \n",
      " 4   floors             32730 non-null  int64  \n",
      " 5   cityCode           32730 non-null  int64  \n",
      " 6   cityPartRange      32730 non-null  int64  \n",
      " 7   numPrevOwners      32730 non-null  int64  \n",
      " 8   made               32730 non-null  int64  \n",
      " 9   isNewBuilt         32730 non-null  int64  \n",
      " 10  hasStormProtector  32730 non-null  int64  \n",
      " 11  basement           32730 non-null  int64  \n",
      " 12  attic              32730 non-null  int64  \n",
      " 13  garage             32730 non-null  int64  \n",
      " 14  hasStorageRoom     32730 non-null  int64  \n",
      " 15  hasGuestRoom       32730 non-null  int64  \n",
      " 16  price              32730 non-null  float64\n",
      " 17  is_generated       32730 non-null  int64  \n",
      "dtypes: float64(1), int64(17)\n",
      "memory usage: 4.5 MB\n"
     ]
    }
   ],
   "source": [
    "df_train = load_dict['train_df']\n",
    "df_test = load_dict['test_df']\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>squareMeters</th>\n",
       "      <th>numberOfRooms</th>\n",
       "      <th>hasYard</th>\n",
       "      <th>hasPool</th>\n",
       "      <th>floors</th>\n",
       "      <th>cityCode</th>\n",
       "      <th>cityPartRange</th>\n",
       "      <th>numPrevOwners</th>\n",
       "      <th>made</th>\n",
       "      <th>isNewBuilt</th>\n",
       "      <th>hasStormProtector</th>\n",
       "      <th>basement</th>\n",
       "      <th>attic</th>\n",
       "      <th>garage</th>\n",
       "      <th>hasStorageRoom</th>\n",
       "      <th>hasGuestRoom</th>\n",
       "      <th>price</th>\n",
       "      <th>is_generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75523</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>9373</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4313</td>\n",
       "      <td>9005</td>\n",
       "      <td>956</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7559081.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80771</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>39381</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3653</td>\n",
       "      <td>2436</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8085989.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55712</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>34457</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2937</td>\n",
       "      <td>8852</td>\n",
       "      <td>135</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5574642.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32316</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>27939</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>659</td>\n",
       "      <td>7141</td>\n",
       "      <td>359</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3232561.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70429</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>38045</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8435</td>\n",
       "      <td>2429</td>\n",
       "      <td>292</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7055052.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   squareMeters  numberOfRooms  hasYard  hasPool  floors  cityCode  \\\n",
       "0         75523              3        0        1      63      9373   \n",
       "1         80771             39        1        1      98     39381   \n",
       "2         55712             58        0        1      19     34457   \n",
       "3         32316             47        0        0       6     27939   \n",
       "4         70429             19        1        1      90     38045   \n",
       "\n",
       "   cityPartRange  numPrevOwners  made  isNewBuilt  hasStormProtector  \\\n",
       "0              3              8  2005           0                  1   \n",
       "1              8              6  2015           1                  0   \n",
       "2              6              8  2021           0                  0   \n",
       "3             10              4  2012           0                  1   \n",
       "4              3              7  1990           1                  0   \n",
       "\n",
       "   basement  attic  garage  hasStorageRoom  hasGuestRoom      price  \\\n",
       "0      4313   9005     956               0             7  7559081.5   \n",
       "1      3653   2436     128               1             2  8085989.5   \n",
       "2      2937   8852     135               1             9  5574642.1   \n",
       "3       659   7141     359               0             3  3232561.2   \n",
       "4      8435   2429     292               1             4  7055052.0   \n",
       "\n",
       "   is_generated  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>32730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>15154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original</th>\n",
       "      <td>32730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count\n",
       "train     32730\n",
       "test      15154\n",
       "original  32730"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [len(df_train), len(df_test), len(df_train)],\n",
    "    index=['train', 'test', 'original'],\n",
    "    columns=['count']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing train</th>\n",
       "      <th>missing test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>squareMeters</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numberOfRooms</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasYard</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasPool</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>floors</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cityCode</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cityPartRange</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numPrevOwners</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>made</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isNewBuilt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasStormProtector</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basement</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attic</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garage</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasStorageRoom</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasGuestRoom</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_generated</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   missing train  missing test\n",
       "squareMeters                   0             0\n",
       "numberOfRooms                  0             0\n",
       "hasYard                        0             0\n",
       "hasPool                        0             0\n",
       "floors                         0             0\n",
       "cityCode                       0             0\n",
       "cityPartRange                  0             0\n",
       "numPrevOwners                  0             0\n",
       "made                           0             0\n",
       "isNewBuilt                     0             0\n",
       "hasStormProtector              0             0\n",
       "basement                       0             0\n",
       "attic                          0             0\n",
       "garage                         0             0\n",
       "hasStorageRoom                 0             0\n",
       "hasGuestRoom                   0             0\n",
       "is_generated                   0             0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET = 'price'\n",
    "pd.concat([\n",
    "    pd.DataFrame(df_train.drop(columns=[TARGET]).isnull().sum(), columns=['missing train']),\n",
    "    pd.DataFrame(df_test.isnull().sum(), columns=['missing test']),\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def check_outliers(df, z=3):\n",
    "    df_z_score = stats.zscore(df)\n",
    "    columns = df.columns.tolist()\n",
    "    if TARGET in columns:\n",
    "        columns.remove(TARGET)\n",
    "    for c in columns:\n",
    "        df_outlier = df_z_score.query(f'{c} > 3 | {c} < -3')\n",
    "        if len(df_outlier):\n",
    "            display(df.loc[df_outlier.index][[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>squareMeters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25334</th>\n",
       "      <td>6071330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       squareMeters\n",
       "25334       6071330"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>floors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15659</th>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       floors\n",
       "15659    6000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cityCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10299</th>\n",
       "      <td>201035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11326</th>\n",
       "      <td>491100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12425</th>\n",
       "      <td>465360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20987</th>\n",
       "      <td>200812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30576</th>\n",
       "      <td>146275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30735</th>\n",
       "      <td>200801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32631</th>\n",
       "      <td>200801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cityCode\n",
       "10299    201035\n",
       "11326    491100\n",
       "12425    465360\n",
       "20987    200812\n",
       "30576    146275\n",
       "30735    200801\n",
       "32631    200801"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>made</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12113</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13608</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29124</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29748</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31400</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        made\n",
       "12113  10000\n",
       "13608  10000\n",
       "29124  10000\n",
       "29748  10000\n",
       "31400  10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12107</th>\n",
       "      <td>84333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13995</th>\n",
       "      <td>81851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25068</th>\n",
       "      <td>91992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28926</th>\n",
       "      <td>91978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       basement\n",
       "12107     84333\n",
       "13995     81851\n",
       "25068     91992\n",
       "28926     91978"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13828</th>\n",
       "      <td>71001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14909</th>\n",
       "      <td>71001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22858</th>\n",
       "      <td>71024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23633</th>\n",
       "      <td>96381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23642</th>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27168</th>\n",
       "      <td>71965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29994</th>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       attic\n",
       "13828  71001\n",
       "14909  71001\n",
       "22858  71024\n",
       "23633  96381\n",
       "23642  30000\n",
       "27168  71965\n",
       "29994  30000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>garage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24878</th>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27629</th>\n",
       "      <td>9017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       garage\n",
       "24878    2048\n",
       "27629    9017"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_outliers(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_z_score = stats.zscore(df_train)\n",
    "columns_to_remove = ['squareMeters', 'floors', 'made', 'garage']\n",
    "df_outlier = df_z_score[((df_z_score[columns_to_remove] > 3) | (df_z_score[columns_to_remove] < -3)).max(axis=1)]\n",
    "df_train = df_train.loc[df_train.index.difference(df_outlier.index)]\n",
    "df_train.set_index(df_train.reset_index().index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32721, 18)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x29fdd8640>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.kdeplot(data=df_train, x=TARGET, label='train')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df['allSpace'] = df['squareMeters'] + df['basement'] + df['attic']\n",
    "    df['roomsPerFloor'] = df['numberOfRooms'] / df['floors']\n",
    "    df['age'] = 2022 - df['made']\n",
    "    df['prevOwnersPerYear'] = df['numPrevOwners'] / df['age']\n",
    "    city_code_count = df['cityCode'].value_counts()    \n",
    "    df['numHousesInCityCode'] = df['cityCode'].map(city_code_count)\n",
    "    city_code_space = df.groupby('cityCode')['squareMeters'].sum()\n",
    "    df['sumAllSpaceInCityCode'] = df['cityCode'].map(city_code_space)\n",
    "    return df\n",
    "\n",
    "df_train = add_features(df_train)\n",
    "df_test = add_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>squareMeters</th>\n",
       "      <th>numberOfRooms</th>\n",
       "      <th>hasYard</th>\n",
       "      <th>hasPool</th>\n",
       "      <th>floors</th>\n",
       "      <th>cityCode</th>\n",
       "      <th>cityPartRange</th>\n",
       "      <th>numPrevOwners</th>\n",
       "      <th>made</th>\n",
       "      <th>isNewBuilt</th>\n",
       "      <th>...</th>\n",
       "      <th>hasStorageRoom</th>\n",
       "      <th>hasGuestRoom</th>\n",
       "      <th>price</th>\n",
       "      <th>is_generated</th>\n",
       "      <th>allSpace</th>\n",
       "      <th>roomsPerFloor</th>\n",
       "      <th>age</th>\n",
       "      <th>prevOwnersPerYear</th>\n",
       "      <th>numHousesInCityCode</th>\n",
       "      <th>sumAllSpaceInCityCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75523</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>9373</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7559081.5</td>\n",
       "      <td>0</td>\n",
       "      <td>88841</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>17</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>2</td>\n",
       "      <td>80321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80771</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>39381</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8085989.5</td>\n",
       "      <td>0</td>\n",
       "      <td>86860</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>7</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>5</td>\n",
       "      <td>145004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55712</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>34457</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5574642.1</td>\n",
       "      <td>0</td>\n",
       "      <td>67501</td>\n",
       "      <td>3.052632</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>146159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32316</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>27939</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3232561.2</td>\n",
       "      <td>0</td>\n",
       "      <td>40116</td>\n",
       "      <td>7.833333</td>\n",
       "      <td>10</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>3</td>\n",
       "      <td>73682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70429</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>38045</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7055052.0</td>\n",
       "      <td>0</td>\n",
       "      <td>81293</td>\n",
       "      <td>0.211111</td>\n",
       "      <td>32</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>2</td>\n",
       "      <td>111945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32716</th>\n",
       "      <td>55825</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>12031</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5594137.1</td>\n",
       "      <td>1</td>\n",
       "      <td>61088</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>22</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>9</td>\n",
       "      <td>480827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32717</th>\n",
       "      <td>65870</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>23197</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6594705.0</td>\n",
       "      <td>1</td>\n",
       "      <td>73135</td>\n",
       "      <td>1.795918</td>\n",
       "      <td>7</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>6</td>\n",
       "      <td>410699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32718</th>\n",
       "      <td>93192</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>8539</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9321511.4</td>\n",
       "      <td>1</td>\n",
       "      <td>102859</td>\n",
       "      <td>1.076923</td>\n",
       "      <td>8</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>5</td>\n",
       "      <td>327980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32719</th>\n",
       "      <td>65797</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>23197</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6584708.2</td>\n",
       "      <td>1</td>\n",
       "      <td>73668</td>\n",
       "      <td>0.966292</td>\n",
       "      <td>22</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>6</td>\n",
       "      <td>410699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32720</th>\n",
       "      <td>82244</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>86728</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8231424.8</td>\n",
       "      <td>1</td>\n",
       "      <td>89829</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>4</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>5</td>\n",
       "      <td>274906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32721 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       squareMeters  numberOfRooms  hasYard  hasPool  floors  cityCode  \\\n",
       "0             75523              3        0        1      63      9373   \n",
       "1             80771             39        1        1      98     39381   \n",
       "2             55712             58        0        1      19     34457   \n",
       "3             32316             47        0        0       6     27939   \n",
       "4             70429             19        1        1      90     38045   \n",
       "...             ...            ...      ...      ...     ...       ...   \n",
       "32716         55825             84        1        0      70     12031   \n",
       "32717         65870             88        1        0      49     23197   \n",
       "32718         93192             42        1        0      39      8539   \n",
       "32719         65797             86        1        0      89     23197   \n",
       "32720         82244             18        1        0      38     86728   \n",
       "\n",
       "       cityPartRange  numPrevOwners  made  isNewBuilt  ...  hasStorageRoom  \\\n",
       "0                  3              8  2005           0  ...               0   \n",
       "1                  8              6  2015           1  ...               1   \n",
       "2                  6              8  2021           0  ...               1   \n",
       "3                 10              4  2012           0  ...               0   \n",
       "4                  3              7  1990           1  ...               1   \n",
       "...              ...            ...   ...         ...  ...             ...   \n",
       "32716              3             10  2000           0  ...               0   \n",
       "32717              9              9  2015           0  ...               0   \n",
       "32718             10              5  2014           1  ...               0   \n",
       "32719              2             10  2000           1  ...               0   \n",
       "32720              1              9  2018           1  ...               0   \n",
       "\n",
       "       hasGuestRoom      price  is_generated  allSpace  roomsPerFloor  age  \\\n",
       "0                 7  7559081.5             0     88841       0.047619   17   \n",
       "1                 2  8085989.5             0     86860       0.397959    7   \n",
       "2                 9  5574642.1             0     67501       3.052632    1   \n",
       "3                 3  3232561.2             0     40116       7.833333   10   \n",
       "4                 4  7055052.0             0     81293       0.211111   32   \n",
       "...             ...        ...           ...       ...            ...  ...   \n",
       "32716             0  5594137.1             1     61088       1.200000   22   \n",
       "32717             7  6594705.0             1     73135       1.795918    7   \n",
       "32718             0  9321511.4             1    102859       1.076923    8   \n",
       "32719             0  6584708.2             1     73668       0.966292   22   \n",
       "32720             6  8231424.8             1     89829       0.473684    4   \n",
       "\n",
       "       prevOwnersPerYear  numHousesInCityCode  sumAllSpaceInCityCode  \n",
       "0               0.470588                    2                  80321  \n",
       "1               0.857143                    5                 145004  \n",
       "2               8.000000                    2                 146159  \n",
       "3               0.400000                    3                  73682  \n",
       "4               0.218750                    2                 111945  \n",
       "...                  ...                  ...                    ...  \n",
       "32716           0.454545                    9                 480827  \n",
       "32717           1.285714                    6                 410699  \n",
       "32718           0.625000                    5                 327980  \n",
       "32719           0.454545                    6                 410699  \n",
       "32720           2.250000                    5                 274906  \n",
       "\n",
       "[32721 rows x 24 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing the data.\n",
      "Number of features: 22\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def check_colab():\n",
    "    \"\"\"Function to check if we are running on colab. Install packages if we are.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        IN_COLAB = True\n",
    "        print(\"Running on Colab\")\n",
    "        results = subprocess.run([\"pip\", \"install\", \"-r\", \"requirements.txt\"], check=True, capture_output=True)\n",
    "        # Check if the installation was successful\n",
    "        if results.returncode == 0:\n",
    "            print(\"Installation successful\")\n",
    "            print(\"You may need to restart the runtime for the changes to take effect\")\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "            print(\"Loaded environment variables from .env file\")\n",
    "            subprocess.call(['chmod', '+x', './submit_kaggle.sh'])    \n",
    "\n",
    "        else:\n",
    "            print(\"Installation failed\")\n",
    "            print(results.stdout)\n",
    "        \n",
    "    except:\n",
    "        IN_COLAB = False\n",
    "    return IN_COLAB\n",
    "check_colab();\n",
    "\n",
    "def check_kaggle():\n",
    "    \"\"\"Check if we're running in a Kaggle notebook\"\"\"\n",
    "    if 'KAGGLE_URL_BASE' not in os.environ:\n",
    "        return \n",
    "    try: \n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        secret_value_0 = user_secrets.get_secret(\"KAGGLE_KEY\")\n",
    "        secret_value_1 = user_secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "        os.environ['KAGGLE_USERNAME'] = secret_value_1\n",
    "        os.environ['KAGGLE_KEY'] = secret_value_0\n",
    "    except:\n",
    "        pass\n",
    "    print(\"Running on kaggle.\")    \n",
    "    return True\n",
    "running_on_kaggle = check_kaggle()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    "    TensorDataset,\n",
    "    random_split,\n",
    "    SubsetRandomSampler,\n",
    ")\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "from haversine import haversine\n",
    "\n",
    "TARGET = 'price'\n",
    "COMPETITION = 'playground-series-s3e6'\n",
    "\n",
    "def load_data(target = TARGET,load_original=True):\n",
    "    # Load the data, turn it into tensors\n",
    "    train_df = pd.read_csv('train.csv', index_col='id')\n",
    "    test_df = pd.read_csv('test.csv', index_col='id')\n",
    "\n",
    "    # Add generative flags\n",
    "\n",
    "    if load_original:\n",
    "        test_df['is_generated'] = 1\n",
    "        train_df['is_generated'] = 1\n",
    "        original = pd.read_csv('ParisHousing.csv')\n",
    "        original['is_generated'] = 0\n",
    "        ext_df = original\n",
    "        train_df = pd.concat([ext_df,train_df])\n",
    "        train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    \n",
    "    # Reset index\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    \n",
    "    FEATURES = [col for col in train_df.columns if col not in ['id', target]]\n",
    "\n",
    "\n",
    "    train_tensors = torch.tensor(train_df[FEATURES].values, dtype=torch.float32)\n",
    "    target_tensors = torch.tensor(train_df[target].values, dtype=torch.float32)\n",
    "    test_tensors = torch.tensor(test_df[FEATURES].values, dtype=torch.float32)\n",
    "    output_dict = {\n",
    "        \"train_df\": train_df,\n",
    "        \"test_df\": test_df,\n",
    "        \"train_tensors\": train_tensors,\n",
    "        \"target_tensors\": target_tensors,\n",
    "        \"test_tensors\": test_tensors,\n",
    "    }\n",
    "    return output_dict\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df['allSpace'] = df['squareMeters'] + df['basement'] + df['attic']\n",
    "    df['roomsPerFloor'] = df['numberOfRooms'] / df['floors']\n",
    "    df['age'] = 2022 - df['made']\n",
    "    df['prevOwnersPerYear'] = df['numPrevOwners'] / df['age']\n",
    "    city_code_count = df['cityCode'].value_counts()    \n",
    "    df['numHousesInCityCode'] = df['cityCode'].map(city_code_count)\n",
    "    city_code_space = df.groupby('cityCode')['squareMeters'].sum()\n",
    "    df['sumAllSpaceInCityCode'] = df['cityCode'].map(city_code_space)\n",
    "    return df\n",
    "\n",
    "def remove_outliers(df_train,z_score = 3):\n",
    "    df_z_score = stats.zscore(df_train)\n",
    "    columns_to_remove = ['squareMeters', 'floors', 'made', 'garage']\n",
    "    df_outlier = df_z_score[((df_z_score[columns_to_remove] > 3) | (df_z_score[columns_to_remove] < -3)).max(axis=1)]\n",
    "    df_train = df_train.loc[df_train.index.difference(df_outlier.index)]\n",
    "    df_train.set_index(df_train.reset_index().index, inplace=True)\n",
    "    return df_train\n",
    "\n",
    "def sklearn_standardize(train_df,test_df,target = TARGET):\n",
    "    # Standardize the data\n",
    "    print(\"Standardizing the data.\")\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_df.drop(target,axis=1))\n",
    "    x_df = train_df.drop(target,axis=1)\n",
    "    x_df = scaler.transform(x_df)\n",
    "    x_df = pd.DataFrame(x_df,columns=train_df.drop(target,axis=1).columns)\n",
    "    y_df = train_df[target]\n",
    "    y_df = pd.DataFrame(y_df,columns=[target])\n",
    "    x_test_df = test_df.copy()\n",
    "    x_test_df = scaler.transform(x_test_df)\n",
    "    x_test_df = pd.DataFrame(x_test_df,columns=test_df.columns)\n",
    "    return x_df, y_df, x_test_df\n",
    "\n",
    "\n",
    "def preprocess(train_df,test_df):\n",
    "    # Preprocess the data\n",
    "    # Remove outliers\n",
    "    train_df = remove_outliers(train_df)\n",
    "    # Feature engineering\n",
    "    train_df = feature_engineering(train_df)\n",
    "    test_df = feature_engineering(test_df)\n",
    "    # Standardize the data\n",
    "    x_df, y_df, x_test_df = sklearn_standardize(train_df,test_df)\n",
    "    return x_df, y_df, x_test_df\n",
    "\n",
    "def main_load(load_original = True):\n",
    "    # Load the data\n",
    "    data_dict = load_data(TARGET,load_original)\n",
    "    train_df = data_dict['train_df']\n",
    "    test_df = data_dict['test_df']\n",
    "    train_tensors = data_dict['train_tensors']\n",
    "    target_tensors = data_dict['target_tensors']\n",
    "    test_tensors = data_dict['test_tensors']\n",
    "\n",
    "    # Preprocess the data\n",
    "    x_df, y_df, x_test_df = preprocess(train_df,test_df)\n",
    "\n",
    "    # Convert to tensors\n",
    "    train_tensors = torch.from_numpy(x_df.values).float()\n",
    "    target_tensors = torch.from_numpy(y_df.values).float().squeeze(1)\n",
    "    test_tensors = torch.from_numpy(x_test_df.values).float()\n",
    "\n",
    "    print(\"Number of features:\", train_tensors.shape[1])\n",
    "\n",
    "    # Create the dataset\n",
    "    train_dataset = TensorDataset(train_tensors, target_tensors)\n",
    "    test_dataset = TensorDataset(test_tensors)\n",
    "\n",
    "    # Create test dataloader\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "    return train_dataset, test_dataset, test_loader,x_df\n",
    "\n",
    "train_dataset,test_dataset,test_loader,x_df = main_load(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "--------------------------------\n",
      "Epoch: 0\t Validation Loss: 30402046707484.445\n",
      "Epoch: 1\t Validation Loss: 30384936510350.223\n",
      "Epoch: 2\t Validation Loss: 30448496294115.555\n",
      "Epoch: 3\t Validation Loss: 30418665472000.0\n",
      "Epoch: 4\t Validation Loss: 30378011248412.445\n",
      "Epoch: 5\t Validation Loss: 30379351095523.555\n",
      "Epoch: 6\t Validation Loss: 30342360829496.89\n",
      "Epoch: 7\t Validation Loss: 30376349371960.89\n",
      "Epoch: 8\t Validation Loss: 30331919575722.668\n",
      "Epoch: 9\t Validation Loss: 30316221926968.89\n",
      "Epoch: 10\t Validation Loss: 30332131388074.668\n",
      "Epoch: 11\t Validation Loss: 30349904518257.777\n",
      "Epoch: 12\t Validation Loss: 30268213457351.11\n",
      "Epoch: 13\t Validation Loss: 30258046464455.11\n",
      "Epoch: 14\t Validation Loss: 30219309969863.11\n",
      "Epoch: 15\t Validation Loss: 30182005830087.11\n",
      "Epoch: 16\t Validation Loss: 30178395932444.445\n",
      "Epoch: 17\t Validation Loss: 30209096140572.445\n",
      "Epoch: 18\t Validation Loss: 30139189209770.668\n",
      "Epoch: 19\t Validation Loss: 30086875287096.89\n",
      "Epoch: 20\t Validation Loss: 30068613287480.89\n",
      "Epoch: 21\t Validation Loss: 30080449380352.0\n",
      "Epoch: 22\t Validation Loss: 30045564654933.332\n",
      "Epoch: 23\t Validation Loss: 29999289131918.223\n",
      "Epoch: 24\t Validation Loss: 29945969042318.223\n",
      "Epoch: 25\t Validation Loss: 29909786645845.332\n",
      "Epoch: 26\t Validation Loss: 29948046387882.668\n",
      "Epoch: 27\t Validation Loss: 29838051930567.11\n",
      "Epoch: 28\t Validation Loss: 29828044787256.89\n",
      "Epoch: 29\t Validation Loss: 29742213715285.332\n",
      "Epoch: 30\t Validation Loss: 29770021950805.332\n",
      "Epoch: 31\t Validation Loss: 29723407388216.89\n",
      "Epoch: 32\t Validation Loss: 29679210608867.555\n",
      "Epoch: 33\t Validation Loss: 29616200045909.332\n",
      "Epoch: 34\t Validation Loss: 29642666104149.332\n",
      "Epoch: 35\t Validation Loss: 29505254879687.11\n",
      "Epoch: 36\t Validation Loss: 29468418637824.0\n",
      "Epoch: 37\t Validation Loss: 29440151126016.0\n",
      "Epoch: 38\t Validation Loss: 29496109898865.777\n",
      "Epoch: 39\t Validation Loss: 29416771376469.332\n",
      "Epoch: 40\t Validation Loss: 29371587537578.668\n",
      "Epoch: 41\t Validation Loss: 29311468714211.555\n",
      "Epoch: 42\t Validation Loss: 29207062001891.555\n",
      "Epoch: 43\t Validation Loss: 29180938478478.223\n",
      "Epoch: 44\t Validation Loss: 29162541562083.555\n",
      "Epoch: 45\t Validation Loss: 29096202352867.555\n",
      "Epoch: 46\t Validation Loss: 29193288140572.445\n",
      "Epoch: 47\t Validation Loss: 28978601059214.223\n",
      "Epoch: 48\t Validation Loss: 28945067365717.332\n",
      "Epoch: 49\t Validation Loss: 28878017688007.11\n",
      "Epoch: 50\t Validation Loss: 28844362592711.11\n",
      "Epoch: 51\t Validation Loss: 28753081022691.555\n",
      "Epoch: 52\t Validation Loss: 28728980318890.668\n",
      "Epoch: 53\t Validation Loss: 28631315018183.11\n",
      "Epoch: 54\t Validation Loss: 28661193841777.777\n",
      "Epoch: 55\t Validation Loss: 28565652956046.223\n",
      "Epoch: 56\t Validation Loss: 28455647546936.89\n",
      "Epoch: 57\t Validation Loss: 28389217927168.0\n",
      "Epoch: 58\t Validation Loss: 28451479340828.445\n",
      "Epoch: 59\t Validation Loss: 28270524017322.668\n",
      "Epoch: 60\t Validation Loss: 28265709888398.223\n",
      "Epoch: 61\t Validation Loss: 28234678097237.332\n",
      "Epoch: 62\t Validation Loss: 28144840998912.0\n",
      "Epoch: 63\t Validation Loss: 28041152444643.555\n",
      "Epoch: 64\t Validation Loss: 28082484514360.89\n",
      "Epoch: 65\t Validation Loss: 27943295371036.445\n",
      "Epoch: 66\t Validation Loss: 28052049945486.223\n",
      "Epoch: 67\t Validation Loss: 27880031285703.11\n",
      "Epoch: 68\t Validation Loss: 27822878068280.89\n",
      "Epoch: 69\t Validation Loss: 27775533929813.332\n",
      "Epoch: 70\t Validation Loss: 27624340688440.89\n",
      "Epoch: 71\t Validation Loss: 27587121599829.332\n",
      "Epoch: 72\t Validation Loss: 27550767935943.11\n",
      "Epoch: 73\t Validation Loss: 27407701624604.445\n",
      "Epoch: 74\t Validation Loss: 27444693288732.445\n",
      "Epoch: 75\t Validation Loss: 27495586740451.555\n",
      "Epoch: 76\t Validation Loss: 27259177591694.223\n",
      "Epoch: 77\t Validation Loss: 27154343060366.223\n",
      "Epoch: 78\t Validation Loss: 27011559631530.668\n",
      "Epoch: 79\t Validation Loss: 27180071853169.777\n",
      "Epoch: 80\t Validation Loss: 27114554494520.89\n",
      "Epoch: 81\t Validation Loss: 27018100881635.555\n",
      "Epoch: 82\t Validation Loss: 26738588501788.445\n",
      "Epoch: 83\t Validation Loss: 26753504844913.777\n",
      "Epoch: 84\t Validation Loss: 26606282444344.89\n",
      "Epoch: 85\t Validation Loss: 26641006155093.332\n",
      "Epoch: 86\t Validation Loss: 26707550652188.445\n",
      "Epoch: 87\t Validation Loss: 26529613363427.555\n",
      "Epoch: 88\t Validation Loss: 26389116762339.555\n",
      "Epoch: 89\t Validation Loss: 26189174290659.555\n",
      "Epoch: 90\t Validation Loss: 26250430935495.11\n",
      "Epoch: 91\t Validation Loss: 26098505089024.0\n",
      "Epoch: 92\t Validation Loss: 26056887107584.0\n",
      "Epoch: 93\t Validation Loss: 26034969772032.0\n",
      "Epoch: 94\t Validation Loss: 25902362190734.223\n",
      "Epoch: 95\t Validation Loss: 25812410914133.332\n",
      "Epoch: 96\t Validation Loss: 25702834023082.668\n",
      "Epoch: 97\t Validation Loss: 25673300297500.445\n",
      "Epoch: 98\t Validation Loss: 25525147460494.223\n",
      "Epoch: 99\t Validation Loss: 25501590851128.89\n",
      "Epoch: 100\t Validation Loss: 25620950490225.777\n",
      "Epoch: 101\t Validation Loss: 25360370033550.223\n",
      "Epoch: 102\t Validation Loss: 25245566806698.668\n",
      "Epoch: 103\t Validation Loss: 25140885367466.668\n",
      "Epoch: 104\t Validation Loss: 25226787276572.445\n",
      "Epoch: 105\t Validation Loss: 24929595206314.668\n",
      "Epoch: 106\t Validation Loss: 24831282351672.89\n",
      "Epoch: 107\t Validation Loss: 24948430427477.332\n",
      "Epoch: 108\t Validation Loss: 24813883213596.445\n",
      "Epoch: 109\t Validation Loss: 24576589066695.11\n",
      "Epoch: 110\t Validation Loss: 24521458668885.332\n",
      "Epoch: 111\t Validation Loss: 24270888617756.445\n",
      "Epoch: 112\t Validation Loss: 24208524443648.0\n",
      "Epoch: 113\t Validation Loss: 24136572789646.223\n",
      "Epoch: 114\t Validation Loss: 24280941432376.89\n",
      "Epoch: 115\t Validation Loss: 24323557891185.777\n",
      "Epoch: 116\t Validation Loss: 23961697575822.223\n",
      "Epoch: 117\t Validation Loss: 23758436943644.445\n",
      "Epoch: 118\t Validation Loss: 24034291424369.777\n",
      "Epoch: 119\t Validation Loss: 23978173734001.777\n",
      "Epoch: 120\t Validation Loss: 23642752213902.223\n",
      "Epoch: 121\t Validation Loss: 23547314321635.555\n",
      "Epoch: 122\t Validation Loss: 23413174421276.445\n",
      "Epoch: 123\t Validation Loss: 23638362641749.332\n",
      "Epoch: 124\t Validation Loss: 23355815682958.223\n",
      "Epoch: 125\t Validation Loss: 23120944678684.445\n",
      "Epoch: 126\t Validation Loss: 23390015571740.445\n",
      "Epoch: 127\t Validation Loss: 23036202727651.555\n",
      "Epoch: 128\t Validation Loss: 22958649813674.668\n",
      "Epoch: 129\t Validation Loss: 22898158629319.11\n",
      "Epoch: 130\t Validation Loss: 22877619821681.777\n",
      "Epoch: 131\t Validation Loss: 22559173348920.89\n",
      "Epoch: 132\t Validation Loss: 22799629768021.332\n",
      "Epoch: 133\t Validation Loss: 22633570981205.332\n",
      "Epoch: 134\t Validation Loss: 22468133417415.11\n",
      "Epoch: 135\t Validation Loss: 22514637296981.332\n",
      "Epoch: 136\t Validation Loss: 22307500428856.89\n",
      "Epoch: 137\t Validation Loss: 22122001975068.445\n",
      "Epoch: 138\t Validation Loss: 22070573982606.223\n",
      "Epoch: 139\t Validation Loss: 21952170644821.332\n",
      "Epoch: 140\t Validation Loss: 21871694300956.445\n",
      "Epoch: 141\t Validation Loss: 21706199415011.555\n",
      "Epoch: 142\t Validation Loss: 21708958567992.89\n",
      "Epoch: 143\t Validation Loss: 21456166467811.555\n",
      "Epoch: 144\t Validation Loss: 21446502092344.89\n",
      "Epoch: 145\t Validation Loss: 21407166744348.445\n",
      "Epoch: 146\t Validation Loss: 21662709142869.332\n",
      "Epoch: 147\t Validation Loss: 20949140591957.332\n",
      "Epoch: 148\t Validation Loss: 21029129697507.555\n",
      "Epoch: 149\t Validation Loss: 20928613901198.223\n",
      "Epoch: 150\t Validation Loss: 20835933297777.777\n",
      "Epoch: 151\t Validation Loss: 20931312236771.555\n",
      "Epoch: 152\t Validation Loss: 20629432099726.223\n",
      "Epoch: 153\t Validation Loss: 20735091510840.89\n",
      "Epoch: 154\t Validation Loss: 20453003362304.0\n",
      "Epoch: 155\t Validation Loss: 20144065143694.223\n",
      "Epoch: 156\t Validation Loss: 20107388751416.89\n",
      "Epoch: 157\t Validation Loss: 20898509983288.89\n",
      "Epoch: 158\t Validation Loss: 19985979029731.555\n",
      "Epoch: 159\t Validation Loss: 20076436419015.11\n",
      "Epoch: 160\t Validation Loss: 19780245622328.89\n",
      "Epoch: 161\t Validation Loss: 19906570136234.668\n",
      "Epoch: 162\t Validation Loss: 19431151952782.223\n",
      "Epoch: 163\t Validation Loss: 19690888422286.223\n",
      "Epoch: 164\t Validation Loss: 19662696058424.89\n",
      "Epoch: 165\t Validation Loss: 19122318242247.11\n",
      "Epoch: 166\t Validation Loss: 19334880908629.332\n",
      "Epoch: 167\t Validation Loss: 19148864341788.445\n",
      "Epoch: 168\t Validation Loss: 19098722952078.223\n",
      "Epoch: 169\t Validation Loss: 18866973791573.332\n",
      "Epoch: 170\t Validation Loss: 18813015119189.332\n",
      "Epoch: 171\t Validation Loss: 19071926126364.445\n",
      "Epoch: 172\t Validation Loss: 18542744072647.11\n",
      "Epoch: 173\t Validation Loss: 19028028890225.777\n",
      "Epoch: 174\t Validation Loss: 18402350465024.0\n",
      "Epoch: 175\t Validation Loss: 18335786627527.11\n",
      "Epoch: 176\t Validation Loss: 17659749757383.11\n",
      "Epoch: 177\t Validation Loss: 18215074907932.445\n",
      "Epoch: 178\t Validation Loss: 18197484696007.11\n",
      "Epoch: 179\t Validation Loss: 17867256772835.555\n",
      "Epoch: 180\t Validation Loss: 17953152273976.89\n",
      "Epoch: 181\t Validation Loss: 17863227677809.777\n",
      "Epoch: 182\t Validation Loss: 17297432262883.555\n",
      "Epoch: 183\t Validation Loss: 17767128367104.0\n",
      "Epoch: 184\t Validation Loss: 17485024606435.555\n",
      "Epoch: 185\t Validation Loss: 17465129857479.111\n",
      "Epoch: 186\t Validation Loss: 17311242591345.777\n",
      "Epoch: 187\t Validation Loss: 17537186136064.0\n",
      "Epoch: 188\t Validation Loss: 16668819339491.555\n",
      "Epoch: 189\t Validation Loss: 17205755379712.0\n",
      "Epoch: 190\t Validation Loss: 16851246514176.0\n",
      "Epoch: 191\t Validation Loss: 16796837886179.555\n",
      "Epoch: 192\t Validation Loss: 16706632155136.0\n",
      "Epoch: 193\t Validation Loss: 15976115586844.445\n",
      "Epoch: 194\t Validation Loss: 16486966434929.777\n",
      "Epoch: 195\t Validation Loss: 16244537644373.334\n",
      "Epoch: 196\t Validation Loss: 16304402828856.889\n",
      "Epoch: 197\t Validation Loss: 16117333491712.0\n",
      "Epoch: 198\t Validation Loss: 15877119410176.0\n",
      "Epoch: 199\t Validation Loss: 15820984476558.223\n",
      "Epoch: 200\t Validation Loss: 15768733351936.0\n",
      "Epoch: 201\t Validation Loss: 15653031029418.666\n",
      "Epoch: 202\t Validation Loss: 15752072061838.223\n",
      "Epoch: 203\t Validation Loss: 15040124609422.223\n",
      "Epoch: 204\t Validation Loss: 15202538313045.334\n",
      "Epoch: 205\t Validation Loss: 15543676786005.334\n",
      "Epoch: 206\t Validation Loss: 15098549283498.666\n",
      "Epoch: 207\t Validation Loss: 15296579618588.445\n",
      "Epoch: 208\t Validation Loss: 14801735283143.111\n",
      "Epoch: 209\t Validation Loss: 14852511760384.0\n",
      "Epoch: 210\t Validation Loss: 14673002772707.555\n",
      "Epoch: 211\t Validation Loss: 14470044712960.0\n",
      "Epoch: 212\t Validation Loss: 14652549249251.555\n",
      "Epoch: 213\t Validation Loss: 14319206822343.111\n",
      "Epoch: 214\t Validation Loss: 14046596402744.889\n",
      "Epoch: 215\t Validation Loss: 14059980193792.0\n",
      "Epoch: 216\t Validation Loss: 13906498493553.777\n",
      "Epoch: 217\t Validation Loss: 13984356310129.777\n",
      "Epoch: 218\t Validation Loss: 13934643904512.0\n",
      "Epoch: 219\t Validation Loss: 13712366880540.445\n",
      "Epoch: 220\t Validation Loss: 13701160632320.0\n",
      "Epoch: 221\t Validation Loss: 13312012291640.889\n",
      "Epoch: 222\t Validation Loss: 13543848949987.555\n",
      "Epoch: 223\t Validation Loss: 13479255503303.111\n",
      "Epoch: 224\t Validation Loss: 13162592308792.889\n",
      "Epoch: 225\t Validation Loss: 13233796095089.777\n",
      "Epoch: 226\t Validation Loss: 12811087147463.111\n",
      "Epoch: 227\t Validation Loss: 12696789683768.889\n",
      "Epoch: 228\t Validation Loss: 12856438642005.334\n",
      "Epoch: 229\t Validation Loss: 13064522723783.111\n",
      "Epoch: 230\t Validation Loss: 12646682547541.334\n",
      "Epoch: 231\t Validation Loss: 12058622718407.111\n",
      "Epoch: 232\t Validation Loss: 12378794564721.777\n",
      "Epoch: 233\t Validation Loss: 12138782741845.334\n",
      "Epoch: 234\t Validation Loss: 12376717219157.334\n",
      "Epoch: 235\t Validation Loss: 12080528286606.223\n",
      "Epoch: 236\t Validation Loss: 12073302782407.111\n",
      "Epoch: 237\t Validation Loss: 11788230619591.111\n",
      "Epoch: 238\t Validation Loss: 11661456150072.889\n",
      "Epoch: 239\t Validation Loss: 11809601530538.666\n",
      "Epoch: 240\t Validation Loss: 11748903776711.111\n",
      "Epoch: 241\t Validation Loss: 11429016327509.334\n",
      "Epoch: 242\t Validation Loss: 11016605211761.777\n",
      "Epoch: 243\t Validation Loss: 11187269344369.777\n",
      "Epoch: 244\t Validation Loss: 11322061692017.777\n",
      "Epoch: 245\t Validation Loss: 10915255098481.777\n",
      "Epoch: 246\t Validation Loss: 11317780239701.334\n",
      "Epoch: 247\t Validation Loss: 10777467901269.334\n",
      "Epoch: 248\t Validation Loss: 10523661245553.777\n",
      "Epoch: 249\t Validation Loss: 10616185611150.223\n",
      "Epoch: 250\t Validation Loss: 10482468053902.223\n",
      "Epoch: 251\t Validation Loss: 10218505706609.777\n",
      "Epoch: 252\t Validation Loss: 10145801524565.334\n",
      "Epoch: 253\t Validation Loss: 9950414183537.777\n",
      "Epoch: 254\t Validation Loss: 9926823320689.777\n",
      "Epoch: 255\t Validation Loss: 9871561268337.777\n",
      "Epoch: 256\t Validation Loss: 10364097688007.111\n",
      "Epoch: 257\t Validation Loss: 9840766571861.334\n",
      "Epoch: 258\t Validation Loss: 10122726677617.777\n",
      "Epoch: 259\t Validation Loss: 9470178841941.334\n",
      "Epoch: 260\t Validation Loss: 9629797158456.889\n",
      "Epoch: 261\t Validation Loss: 9132999850211.555\n",
      "Epoch: 262\t Validation Loss: 9046854613219.555\n",
      "Epoch: 263\t Validation Loss: 9162176885191.111\n",
      "Epoch: 264\t Validation Loss: 8862244711082.666\n",
      "Epoch: 265\t Validation Loss: 9109983548757.334\n",
      "Epoch: 266\t Validation Loss: 8796970563811.555\n",
      "Epoch: 267\t Validation Loss: 8894623339861.334\n",
      "Epoch: 268\t Validation Loss: 8931767047509.334\n",
      "Epoch: 269\t Validation Loss: 8582339289998.223\n",
      "Epoch: 270\t Validation Loss: 8487992149788.444\n",
      "Epoch: 271\t Validation Loss: 8530172400981.333\n",
      "Epoch: 272\t Validation Loss: 8586504175616.0\n",
      "Epoch: 273\t Validation Loss: 7999941771264.0\n",
      "Epoch: 274\t Validation Loss: 8621973773425.777\n",
      "Epoch: 275\t Validation Loss: 7946776055352.889\n",
      "Epoch: 276\t Validation Loss: 8010559360568.889\n",
      "Epoch: 277\t Validation Loss: 7878823320689.777\n",
      "Epoch: 278\t Validation Loss: 7807768140913.777\n",
      "Epoch: 279\t Validation Loss: 7520252388238.223\n",
      "Epoch: 280\t Validation Loss: 7552691543153.777\n",
      "Epoch: 281\t Validation Loss: 7430949015096.889\n",
      "Epoch: 282\t Validation Loss: 7476906237041.777\n",
      "Epoch: 283\t Validation Loss: 7514206998072.889\n",
      "Epoch: 284\t Validation Loss: 7283814927928.889\n",
      "Epoch: 285\t Validation Loss: 7208644378624.0\n",
      "Epoch: 286\t Validation Loss: 7272601980472.889\n",
      "Epoch: 287\t Validation Loss: 7341031467690.667\n",
      "Epoch: 288\t Validation Loss: 6881350021575.111\n",
      "Epoch: 289\t Validation Loss: 6806724964807.111\n",
      "Epoch: 290\t Validation Loss: 6961692205966.223\n",
      "Epoch: 291\t Validation Loss: 6459231189219.556\n",
      "Epoch: 292\t Validation Loss: 6547754654833.777\n",
      "Epoch: 293\t Validation Loss: 6546577628273.777\n",
      "Epoch: 294\t Validation Loss: 6355800002104.889\n",
      "Epoch: 295\t Validation Loss: 6263227402012.444\n",
      "Epoch: 296\t Validation Loss: 5910294720967.111\n",
      "Epoch: 297\t Validation Loss: 6039544936675.556\n",
      "Epoch: 298\t Validation Loss: 6501166772679.111\n",
      "Epoch: 299\t Validation Loss: 5700574161578.667\n",
      "Epoch: 300\t Validation Loss: 5709150231665.777\n",
      "Epoch: 301\t Validation Loss: 5689020659939.556\n",
      "Epoch: 302\t Validation Loss: 5903301651114.667\n",
      "Epoch: 303\t Validation Loss: 5663123803704.889\n",
      "Epoch: 304\t Validation Loss: 5459143149340.444\n",
      "Epoch: 305\t Validation Loss: 5385853920142.223\n",
      "Epoch: 306\t Validation Loss: 5453063214421.333\n",
      "Epoch: 307\t Validation Loss: 5296444000028.444\n",
      "Epoch: 308\t Validation Loss: 5115012156984.889\n",
      "Epoch: 309\t Validation Loss: 4886404064142.223\n",
      "Epoch: 310\t Validation Loss: 5074815578567.111\n",
      "Epoch: 311\t Validation Loss: 4743653374179.556\n",
      "Epoch: 312\t Validation Loss: 4993721798200.889\n",
      "Epoch: 313\t Validation Loss: 4674710609009.777\n",
      "Epoch: 314\t Validation Loss: 4550589852330.667\n",
      "Epoch: 315\t Validation Loss: 4715340365824.0\n",
      "Epoch: 316\t Validation Loss: 4400271793265.777\n",
      "Epoch: 317\t Validation Loss: 4192507352405.3335\n",
      "Epoch: 318\t Validation Loss: 4385031993571.5557\n",
      "Epoch: 319\t Validation Loss: 4395288756224.0\n",
      "Epoch: 320\t Validation Loss: 4144775240817.778\n",
      "Epoch: 321\t Validation Loss: 4046340984376.8887\n",
      "Epoch: 322\t Validation Loss: 4078874307242.6665\n",
      "Epoch: 323\t Validation Loss: 4076479913073.778\n",
      "Epoch: 324\t Validation Loss: 3783524555889.778\n",
      "Epoch: 325\t Validation Loss: 3780796976696.8887\n",
      "Epoch: 326\t Validation Loss: 3405859404003.5557\n",
      "Epoch: 327\t Validation Loss: 3797201336547.5557\n",
      "Epoch: 328\t Validation Loss: 3664596930104.8887\n",
      "Epoch: 329\t Validation Loss: 3522767152014.222\n",
      "Epoch: 330\t Validation Loss: 3744741370538.6665\n",
      "Epoch: 331\t Validation Loss: 3352363765304.8887\n",
      "Epoch: 332\t Validation Loss: 3227042913393.778\n",
      "Epoch: 333\t Validation Loss: 3187516491548.4443\n",
      "Epoch: 334\t Validation Loss: 3014652854272.0\n",
      "Epoch: 335\t Validation Loss: 3019241568483.5557\n",
      "Epoch: 336\t Validation Loss: 3337813491712.0\n",
      "Epoch: 337\t Validation Loss: 3007656813454.222\n",
      "Epoch: 338\t Validation Loss: 3119779355761.778\n",
      "Epoch: 339\t Validation Loss: 3089840472064.0\n",
      "Epoch: 340\t Validation Loss: 2931483942001.778\n",
      "Epoch: 341\t Validation Loss: 2759254897095.1113\n",
      "Epoch: 342\t Validation Loss: 2810303508935.1113\n",
      "Epoch: 343\t Validation Loss: 2707626597489.778\n",
      "Epoch: 344\t Validation Loss: 2718117862513.778\n",
      "Epoch: 345\t Validation Loss: 2410093215744.0\n",
      "Epoch: 346\t Validation Loss: 2577725419975.1113\n",
      "Epoch: 347\t Validation Loss: 2550156259783.1113\n",
      "Epoch: 348\t Validation Loss: 2318277629269.3335\n",
      "Epoch: 349\t Validation Loss: 2669177121450.6665\n",
      "Epoch: 350\t Validation Loss: 2112976089543.111\n",
      "Epoch: 351\t Validation Loss: 2507441467847.1113\n",
      "Epoch: 352\t Validation Loss: 2201067803989.3335\n",
      "Epoch: 353\t Validation Loss: 2226817087715.5557\n",
      "Epoch: 354\t Validation Loss: 2070878370019.5557\n",
      "Epoch: 355\t Validation Loss: 1898919304305.7778\n",
      "Epoch: 356\t Validation Loss: 1781148651064.889\n",
      "Epoch: 357\t Validation Loss: 1895131075925.3333\n",
      "Epoch: 358\t Validation Loss: 1925309005824.0\n",
      "Epoch: 359\t Validation Loss: 1714642339612.4443\n",
      "Epoch: 360\t Validation Loss: 1716683873393.7778\n",
      "Epoch: 361\t Validation Loss: 2010743395669.3333\n",
      "Epoch: 362\t Validation Loss: 1851925200896.0\n",
      "Epoch: 363\t Validation Loss: 1763288882289.7778\n",
      "Epoch: 364\t Validation Loss: 1734624528156.4443\n",
      "Epoch: 365\t Validation Loss: 1537778181006.2222\n",
      "Epoch: 366\t Validation Loss: 1543781555313.7778\n",
      "Epoch: 367\t Validation Loss: 1479358298339.5557\n",
      "Epoch: 368\t Validation Loss: 1311387345806.2222\n",
      "Epoch: 369\t Validation Loss: 1510903731541.3333\n",
      "Epoch: 370\t Validation Loss: 1541240127488.0\n",
      "Epoch: 371\t Validation Loss: 1163620464867.5557\n",
      "Epoch: 372\t Validation Loss: 1405588486371.5557\n",
      "Epoch: 373\t Validation Loss: 1387878867854.2222\n",
      "Epoch: 374\t Validation Loss: 1120714184021.3333\n",
      "Epoch: 375\t Validation Loss: 1110971077973.3333\n",
      "Epoch: 376\t Validation Loss: 1033187207850.6666\n",
      "Epoch: 377\t Validation Loss: 1234362404408.889\n",
      "Epoch: 378\t Validation Loss: 1173034951566.2222\n",
      "Epoch: 379\t Validation Loss: 993147275036.4445\n",
      "Epoch: 380\t Validation Loss: 1134449379100.4443\n",
      "Epoch: 381\t Validation Loss: 971468920149.3334\n",
      "Epoch: 382\t Validation Loss: 1039756470954.6666\n",
      "Epoch: 383\t Validation Loss: 859767737912.8889\n",
      "Epoch: 384\t Validation Loss: 674231236835.5555\n",
      "Epoch: 385\t Validation Loss: 786028880782.2222\n",
      "Epoch: 386\t Validation Loss: 907647735125.3334\n",
      "Epoch: 387\t Validation Loss: 769669820871.1111\n",
      "Epoch: 388\t Validation Loss: 688223449543.1111\n",
      "Epoch: 389\t Validation Loss: 711699770026.6666\n",
      "Epoch: 390\t Validation Loss: 756174851640.8889\n",
      "Epoch: 391\t Validation Loss: 629918582556.4445\n",
      "Epoch: 392\t Validation Loss: 626722785052.4445\n",
      "Epoch: 393\t Validation Loss: 501858861056.0\n",
      "Epoch: 394\t Validation Loss: 844007604224.0\n",
      "Epoch: 395\t Validation Loss: 666468970951.1111\n",
      "Epoch: 396\t Validation Loss: 504045498823.1111\n",
      "Epoch: 397\t Validation Loss: 564652569031.1111\n",
      "Epoch: 398\t Validation Loss: 573648717596.4445\n",
      "Epoch: 399\t Validation Loss: 656384065536.0\n",
      "Epoch: 400\t Validation Loss: 524472775111.1111\n",
      "Epoch: 401\t Validation Loss: 477189585123.55554\n",
      "Epoch: 402\t Validation Loss: 445319883434.6667\n",
      "Epoch: 403\t Validation Loss: 530669720917.3333\n",
      "Epoch: 404\t Validation Loss: 408032338830.2222\n",
      "Epoch: 405\t Validation Loss: 311098736640.0\n",
      "Epoch: 406\t Validation Loss: 383437730247.1111\n",
      "Epoch: 407\t Validation Loss: 287588484892.44446\n",
      "Epoch: 408\t Validation Loss: 318792699448.8889\n",
      "Epoch: 409\t Validation Loss: 400043671552.0\n",
      "Epoch: 410\t Validation Loss: 314790474183.1111\n",
      "Epoch: 411\t Validation Loss: 375219308316.44446\n",
      "Epoch: 412\t Validation Loss: 339221898126.2222\n",
      "Epoch: 413\t Validation Loss: 261915255694.22223\n",
      "Epoch: 414\t Validation Loss: 240715477447.1111\n",
      "Epoch: 415\t Validation Loss: 216125514638.22223\n",
      "Epoch: 416\t Validation Loss: 193062234794.66666\n",
      "Epoch: 417\t Validation Loss: 217682447928.8889\n",
      "Epoch: 418\t Validation Loss: 188865713493.33334\n",
      "Epoch: 419\t Validation Loss: 260517340046.22223\n",
      "Epoch: 420\t Validation Loss: 209173827128.8889\n",
      "Epoch: 421\t Validation Loss: 140069534833.77777\n",
      "Epoch: 422\t Validation Loss: 159563576661.33334\n",
      "Epoch: 423\t Validation Loss: 219566484138.66666\n",
      "Epoch: 424\t Validation Loss: 132601968867.55556\n",
      "Epoch: 425\t Validation Loss: 164533897898.66666\n",
      "Epoch: 426\t Validation Loss: 132869452231.11111\n",
      "Epoch: 427\t Validation Loss: 138384966542.22223\n",
      "Epoch: 428\t Validation Loss: 180898853319.1111\n",
      "Epoch: 429\t Validation Loss: 131104991004.44444\n",
      "Epoch: 430\t Validation Loss: 118354762865.77777\n",
      "Epoch: 431\t Validation Loss: 199281972565.33334\n",
      "Epoch: 432\t Validation Loss: 95474798136.88889\n",
      "Epoch: 433\t Validation Loss: 75156881180.44444\n",
      "Epoch: 434\t Validation Loss: 150812021191.1111\n",
      "Epoch: 435\t Validation Loss: 85849963633.77777\n",
      "Epoch: 436\t Validation Loss: 100413485511.11111\n",
      "Epoch: 437\t Validation Loss: 119418372096.0\n",
      "Epoch: 438\t Validation Loss: 113820987847.11111\n",
      "Epoch: 439\t Validation Loss: 83103502791.11111\n",
      "Epoch: 440\t Validation Loss: 99904453290.66667\n",
      "Epoch: 441\t Validation Loss: 68814324394.66667\n",
      "Epoch: 442\t Validation Loss: 252787205006.22223\n",
      "Epoch: 443\t Validation Loss: 124717723192.88889\n",
      "Epoch: 444\t Validation Loss: 258880696775.1111\n",
      "Epoch: 445\t Validation Loss: 143122272711.1111\n",
      "Epoch: 446\t Validation Loss: 90961570474.66667\n",
      "Epoch: 447\t Validation Loss: 87144107804.44444\n",
      "Epoch: 448\t Validation Loss: 81567678919.11111\n",
      "Epoch: 449\t Validation Loss: 85780490012.44444\n",
      "Epoch: 450\t Validation Loss: 66411436487.111115\n",
      "Epoch: 451\t Validation Loss: 142735529301.33334\n",
      "Epoch: 452\t Validation Loss: 72948416284.44444\n",
      "Epoch: 453\t Validation Loss: 129537804060.44444\n",
      "Epoch: 454\t Validation Loss: 90445210055.11111\n",
      "Epoch: 455\t Validation Loss: 94158579029.33333\n",
      "Epoch: 456\t Validation Loss: 63890046293.333336\n",
      "Epoch: 457\t Validation Loss: 89710653440.0\n",
      "Epoch: 458\t Validation Loss: 54461755278.22222\n",
      "Epoch: 459\t Validation Loss: 264266237724.44446\n",
      "Epoch: 460\t Validation Loss: 87503996700.44444\n",
      "Epoch: 461\t Validation Loss: 64634747221.333336\n",
      "Epoch: 462\t Validation Loss: 54171086506.666664\n",
      "Epoch: 463\t Validation Loss: 74814029368.88889\n",
      "Epoch: 464\t Validation Loss: 55908837717.333336\n",
      "Epoch: 465\t Validation Loss: 65373685987.55556\n",
      "Epoch: 466\t Validation Loss: 75064119523.55556\n",
      "Epoch: 467\t Validation Loss: 60141167957.333336\n",
      "Epoch: 468\t Validation Loss: 108822542108.44444\n",
      "Epoch: 469\t Validation Loss: 100107170702.22223\n",
      "Epoch: 470\t Validation Loss: 96198836679.11111\n",
      "Epoch: 471\t Validation Loss: 73927768064.0\n",
      "Epoch: 472\t Validation Loss: 69920596878.22223\n",
      "Epoch 00474: reducing learning rate of group 0 to 5.0000e-03.\n",
      "Epoch: 473\t Validation Loss: 61314285681.77778\n",
      "Epoch: 474\t Validation Loss: 65807167943.111115\n",
      "Epoch: 475\t Validation Loss: 62516321848.888885\n",
      "Epoch: 476\t Validation Loss: 57240984576.0\n",
      "Epoch: 477\t Validation Loss: 57185821354.666664\n",
      "Early stopping\n",
      "--------------------------------\n",
      "Fold 1\n",
      "--------------------------------\n",
      "Epoch: 0\t Validation Loss: 30066058956344.89\n",
      "Epoch: 1\t Validation Loss: 30038074793073.777\n",
      "Epoch: 2\t Validation Loss: 29995508898929.777\n",
      "Epoch: 3\t Validation Loss: 30026910720910.223\n",
      "Epoch: 4\t Validation Loss: 29991436462762.668\n",
      "Epoch: 5\t Validation Loss: 29981325859953.777\n",
      "Epoch: 6\t Validation Loss: 30013767403292.445\n",
      "Epoch: 7\t Validation Loss: 29928556622279.11\n",
      "Epoch: 8\t Validation Loss: 29966985068544.0\n",
      "Epoch: 9\t Validation Loss: 29936884412871.11\n",
      "Epoch: 10\t Validation Loss: 29933832357660.445\n",
      "Epoch: 11\t Validation Loss: 29920375166293.332\n",
      "Epoch: 12\t Validation Loss: 29946641529059.555\n",
      "Epoch: 13\t Validation Loss: 29953951268864.0\n",
      "Epoch: 14\t Validation Loss: 29891355942001.777\n",
      "Epoch: 15\t Validation Loss: 29862850752967.11\n",
      "Epoch: 16\t Validation Loss: 29847384723000.89\n",
      "Epoch: 17\t Validation Loss: 29795162608981.332\n",
      "Epoch: 18\t Validation Loss: 29746085989944.89\n",
      "Epoch: 19\t Validation Loss: 29771619980629.332\n",
      "Epoch: 20\t Validation Loss: 29728538187093.332\n",
      "Epoch: 21\t Validation Loss: 29668143937763.555\n",
      "Epoch: 22\t Validation Loss: 29650065089422.223\n",
      "Epoch: 23\t Validation Loss: 29696105498396.445\n",
      "Epoch: 24\t Validation Loss: 29510980570680.89\n",
      "Epoch: 25\t Validation Loss: 29569591075726.223\n",
      "Epoch: 26\t Validation Loss: 29541123868444.445\n",
      "Epoch: 27\t Validation Loss: 29516775700707.555\n",
      "Epoch: 28\t Validation Loss: 29482148225934.223\n",
      "Epoch: 29\t Validation Loss: 29427260398705.777\n",
      "Epoch: 30\t Validation Loss: 29422631285191.11\n",
      "Epoch: 31\t Validation Loss: 29315475905649.777\n",
      "Epoch: 32\t Validation Loss: 29353154969600.0\n",
      "Epoch: 33\t Validation Loss: 29220669489152.0\n",
      "Epoch: 34\t Validation Loss: 29168139093788.445\n",
      "Epoch: 35\t Validation Loss: 29146649111210.668\n",
      "Epoch: 36\t Validation Loss: 29123220661134.223\n",
      "Epoch: 37\t Validation Loss: 29133594340010.668\n",
      "Epoch: 38\t Validation Loss: 29075180268202.668\n",
      "Epoch: 39\t Validation Loss: 29051085389824.0\n",
      "Epoch: 40\t Validation Loss: 29091399408753.777\n",
      "Epoch: 41\t Validation Loss: 28897166549902.223\n",
      "Epoch: 42\t Validation Loss: 28895772875889.777\n",
      "Epoch: 43\t Validation Loss: 28880316399616.0\n",
      "Epoch: 44\t Validation Loss: 28724834016369.777\n",
      "Epoch: 45\t Validation Loss: 28707838463544.89\n",
      "Epoch: 46\t Validation Loss: 28622045606343.11\n",
      "Epoch: 47\t Validation Loss: 28679427413333.332\n",
      "Epoch: 48\t Validation Loss: 28604880417223.11\n",
      "Epoch: 49\t Validation Loss: 28524764783502.223\n",
      "Epoch: 50\t Validation Loss: 28519969528945.777\n",
      "Epoch: 51\t Validation Loss: 28401471819320.89\n",
      "Epoch: 52\t Validation Loss: 28241933078072.89\n",
      "Epoch: 53\t Validation Loss: 28289360636586.668\n",
      "Epoch: 54\t Validation Loss: 28220400686421.332\n",
      "Epoch: 55\t Validation Loss: 28227356473571.555\n",
      "Epoch: 56\t Validation Loss: 28179958508202.668\n",
      "Epoch: 57\t Validation Loss: 28123387833002.668\n",
      "Epoch: 58\t Validation Loss: 28073651310136.89\n",
      "Epoch: 59\t Validation Loss: 28105268439722.668\n",
      "Epoch: 60\t Validation Loss: 27881901246236.445\n",
      "Epoch: 61\t Validation Loss: 27798952826197.332\n",
      "Epoch: 62\t Validation Loss: 27880211873792.0\n",
      "Epoch: 63\t Validation Loss: 27812213817344.0\n",
      "Epoch: 64\t Validation Loss: 27759178241365.332\n",
      "Epoch: 65\t Validation Loss: 27519042453504.0\n",
      "Epoch: 66\t Validation Loss: 27511762539861.332\n",
      "Epoch: 67\t Validation Loss: 27516362992298.668\n",
      "Epoch: 68\t Validation Loss: 27451071193998.223\n",
      "Epoch: 69\t Validation Loss: 27372676156984.89\n",
      "Epoch: 70\t Validation Loss: 27385207805269.332\n",
      "Epoch: 71\t Validation Loss: 27159341272632.89\n",
      "Epoch: 72\t Validation Loss: 27203759418026.668\n",
      "Epoch: 73\t Validation Loss: 27130224880298.668\n",
      "Epoch: 74\t Validation Loss: 27044947223438.223\n",
      "Epoch: 75\t Validation Loss: 27070672287971.555\n",
      "Epoch: 76\t Validation Loss: 26962153992647.11\n",
      "Epoch: 77\t Validation Loss: 26914431434752.0\n",
      "Epoch: 78\t Validation Loss: 26755190955121.777\n",
      "Epoch: 79\t Validation Loss: 26624318883612.445\n",
      "Epoch: 80\t Validation Loss: 26642129063480.89\n",
      "Epoch: 81\t Validation Loss: 26814914581845.332\n",
      "Epoch: 82\t Validation Loss: 26771559226481.777\n",
      "Epoch: 83\t Validation Loss: 26471899274353.777\n",
      "Epoch: 84\t Validation Loss: 26243090437461.332\n",
      "Epoch: 85\t Validation Loss: 26170512667079.11\n",
      "Epoch: 86\t Validation Loss: 26229519533852.445\n",
      "Epoch: 87\t Validation Loss: 26113727849358.223\n",
      "Epoch: 88\t Validation Loss: 26112683933696.0\n",
      "Epoch: 89\t Validation Loss: 26141589212728.89\n",
      "Epoch: 90\t Validation Loss: 25608799125504.0\n",
      "Epoch: 91\t Validation Loss: 25959798756693.332\n",
      "Epoch: 92\t Validation Loss: 25864380670862.223\n",
      "Epoch: 93\t Validation Loss: 25807994778055.11\n",
      "Epoch: 94\t Validation Loss: 25508171481088.0\n",
      "Epoch: 95\t Validation Loss: 25750412110506.668\n",
      "Epoch: 96\t Validation Loss: 25279894388736.0\n",
      "Epoch: 97\t Validation Loss: 25450376069120.0\n",
      "Epoch: 98\t Validation Loss: 25417617855829.332\n",
      "Epoch: 99\t Validation Loss: 25101735500913.777\n",
      "Epoch: 100\t Validation Loss: 25041377136184.89\n",
      "Epoch: 101\t Validation Loss: 25128494927416.89\n",
      "Epoch: 102\t Validation Loss: 24990506986154.668\n",
      "Epoch: 103\t Validation Loss: 25106486482261.332\n",
      "Epoch: 104\t Validation Loss: 24728303333831.11\n",
      "Epoch: 105\t Validation Loss: 24796674450318.223\n",
      "Epoch: 106\t Validation Loss: 24482411096746.668\n",
      "Epoch: 107\t Validation Loss: 24345824752071.11\n",
      "Epoch: 108\t Validation Loss: 24542220007651.555\n",
      "Epoch: 109\t Validation Loss: 24445718160270.223\n",
      "Epoch: 110\t Validation Loss: 24452162009315.555\n",
      "Epoch: 111\t Validation Loss: 24238724830549.332\n",
      "Epoch: 112\t Validation Loss: 24111374110264.89\n",
      "Epoch: 113\t Validation Loss: 24120432408803.555\n",
      "Epoch: 114\t Validation Loss: 24225744391736.89\n",
      "Epoch: 115\t Validation Loss: 24141822194119.11\n",
      "Epoch: 116\t Validation Loss: 23688612733838.223\n",
      "Epoch: 117\t Validation Loss: 23788492161024.0\n",
      "Epoch: 118\t Validation Loss: 23517128847815.11\n",
      "Epoch: 119\t Validation Loss: 23451429735992.89\n",
      "Epoch: 120\t Validation Loss: 23372365008440.89\n",
      "Epoch: 121\t Validation Loss: 23361897190741.332\n",
      "Epoch: 122\t Validation Loss: 23270003019320.89\n",
      "Epoch: 123\t Validation Loss: 23000861289130.668\n",
      "Epoch: 124\t Validation Loss: 23085010212181.332\n",
      "Epoch: 125\t Validation Loss: 23004583966947.555\n",
      "Epoch: 126\t Validation Loss: 22775559356416.0\n",
      "Epoch: 127\t Validation Loss: 22879660350577.777\n",
      "Epoch: 128\t Validation Loss: 22794463317560.89\n",
      "Epoch: 129\t Validation Loss: 23033002473699.555\n",
      "Epoch: 130\t Validation Loss: 22389478333553.777\n",
      "Epoch: 131\t Validation Loss: 22528021787079.11\n",
      "Epoch: 132\t Validation Loss: 22663687249009.777\n",
      "Epoch: 133\t Validation Loss: 22710139398826.668\n",
      "Epoch: 134\t Validation Loss: 21900545054037.332\n",
      "Epoch: 135\t Validation Loss: 22276301331569.777\n",
      "Epoch: 136\t Validation Loss: 22573694961436.445\n",
      "Epoch: 137\t Validation Loss: 22146783088184.89\n",
      "Epoch: 138\t Validation Loss: 21529144794225.777\n",
      "Epoch: 139\t Validation Loss: 21997580510094.223\n",
      "Epoch: 140\t Validation Loss: 21935318397383.11\n",
      "Epoch: 141\t Validation Loss: 21475954262016.0\n",
      "Epoch: 142\t Validation Loss: 21218440307598.223\n",
      "Epoch: 143\t Validation Loss: 21705410652842.668\n",
      "Epoch: 144\t Validation Loss: 21136437703111.11\n",
      "Epoch: 145\t Validation Loss: 20751819560277.332\n",
      "Epoch: 146\t Validation Loss: 20975996487452.445\n",
      "Epoch: 147\t Validation Loss: 20536572539335.11\n",
      "Epoch: 148\t Validation Loss: 20660905457891.555\n",
      "Epoch: 149\t Validation Loss: 20744183363811.555\n",
      "Epoch: 150\t Validation Loss: 20344084743964.445\n",
      "Epoch: 151\t Validation Loss: 21294536866019.555\n",
      "Epoch: 152\t Validation Loss: 20237989824284.445\n",
      "Epoch: 153\t Validation Loss: 20135470548764.445\n",
      "Epoch: 154\t Validation Loss: 20514587395868.445\n",
      "Epoch: 155\t Validation Loss: 20198066807694.223\n",
      "Epoch: 156\t Validation Loss: 19824352690176.0\n",
      "Epoch: 157\t Validation Loss: 21225540099185.777\n",
      "Epoch: 158\t Validation Loss: 20276595363384.89\n",
      "Epoch: 159\t Validation Loss: 18983501226894.223\n",
      "Epoch: 160\t Validation Loss: 19737623804131.555\n",
      "Epoch: 161\t Validation Loss: 19224754680263.11\n",
      "Epoch: 162\t Validation Loss: 19244420723143.11\n",
      "Epoch: 163\t Validation Loss: 19847647388558.223\n",
      "Epoch: 164\t Validation Loss: 19217128620032.0\n",
      "Epoch: 165\t Validation Loss: 19448386697443.555\n",
      "Epoch: 166\t Validation Loss: 19606487491470.223\n",
      "Epoch: 167\t Validation Loss: 18657668584789.332\n",
      "Epoch: 168\t Validation Loss: 19644965104298.668\n",
      "Epoch: 169\t Validation Loss: 18528450467157.332\n",
      "Epoch: 170\t Validation Loss: 18700755854222.223\n",
      "Epoch: 171\t Validation Loss: 19235125213411.555\n",
      "Epoch: 172\t Validation Loss: 19069208916423.11\n",
      "Epoch: 173\t Validation Loss: 17815857324942.223\n",
      "Epoch: 174\t Validation Loss: 17610324195555.555\n",
      "Epoch: 175\t Validation Loss: 18035798587619.555\n",
      "Epoch: 176\t Validation Loss: 17671837741511.11\n",
      "Epoch: 177\t Validation Loss: 18226309817230.223\n",
      "Epoch: 178\t Validation Loss: 16060792875690.666\n",
      "Epoch: 179\t Validation Loss: 18655898238976.0\n",
      "Epoch: 180\t Validation Loss: 17022373651342.223\n",
      "Epoch: 181\t Validation Loss: 17384648154225.777\n",
      "Epoch: 182\t Validation Loss: 18161477062200.89\n",
      "Epoch: 183\t Validation Loss: 16532900122168.889\n",
      "Epoch: 184\t Validation Loss: 17397256698083.555\n",
      "Epoch: 185\t Validation Loss: 16280089963178.666\n",
      "Epoch: 186\t Validation Loss: 17805394866631.11\n",
      "Epoch: 187\t Validation Loss: 17027000434688.0\n",
      "Epoch: 188\t Validation Loss: 17592582755669.332\n",
      "Epoch 00190: reducing learning rate of group 0 to 5.0000e-03.\n",
      "Epoch: 189\t Validation Loss: 16084914434503.111\n",
      "Epoch: 190\t Validation Loss: 17053391694506.666\n",
      "Epoch: 191\t Validation Loss: 16754613245269.334\n",
      "Epoch: 192\t Validation Loss: 16381035442631.111\n",
      "Epoch: 193\t Validation Loss: 16314506790684.445\n",
      "Early stopping\n",
      "--------------------------------\n",
      "Fold 2\n",
      "--------------------------------\n",
      "Epoch: 0\t Validation Loss: 29660742622321.777\n",
      "Epoch: 1\t Validation Loss: 29658437852273.777\n",
      "Epoch: 2\t Validation Loss: 29674965507185.777\n",
      "Epoch: 3\t Validation Loss: 29673044515953.777\n",
      "Epoch: 4\t Validation Loss: 29681305663715.555\n",
      "Epoch: 5\t Validation Loss: 29704205165454.223\n",
      "Epoch: 6\t Validation Loss: 29603468236117.332\n",
      "Epoch: 7\t Validation Loss: 29643788546503.11\n",
      "Epoch: 8\t Validation Loss: 29580248802190.223\n",
      "Epoch: 9\t Validation Loss: 29582701304945.777\n",
      "Epoch: 10\t Validation Loss: 29559661294023.11\n",
      "Epoch: 11\t Validation Loss: 29591250461582.223\n",
      "Epoch: 12\t Validation Loss: 29513246892942.223\n",
      "Epoch: 13\t Validation Loss: 29513670517646.223\n",
      "Epoch: 14\t Validation Loss: 29472479423146.668\n",
      "Epoch: 15\t Validation Loss: 29430315250119.11\n",
      "Epoch: 16\t Validation Loss: 29428092268999.11\n",
      "Epoch: 17\t Validation Loss: 29387553854805.332\n",
      "Epoch: 18\t Validation Loss: 29417399823018.668\n",
      "Epoch: 19\t Validation Loss: 29393613692017.777\n",
      "Epoch: 20\t Validation Loss: 29345382924288.0\n",
      "Epoch: 21\t Validation Loss: 29318106899342.223\n",
      "Epoch: 22\t Validation Loss: 29276437188152.89\n",
      "Epoch: 23\t Validation Loss: 29267727016846.223\n",
      "Epoch: 24\t Validation Loss: 29252479090688.0\n",
      "Epoch: 25\t Validation Loss: 29150849473649.777\n",
      "Epoch: 26\t Validation Loss: 29134466522225.777\n",
      "Epoch: 27\t Validation Loss: 29184220055324.445\n",
      "Epoch: 28\t Validation Loss: 29022340193393.777\n",
      "Epoch: 29\t Validation Loss: 29047947118364.445\n",
      "Epoch: 30\t Validation Loss: 29022314095502.223\n",
      "Epoch: 31\t Validation Loss: 28998893101966.223\n",
      "Epoch: 32\t Validation Loss: 28945217661610.668\n",
      "Epoch: 33\t Validation Loss: 28853628975331.555\n",
      "Epoch: 34\t Validation Loss: 28894367784049.777\n",
      "Epoch: 35\t Validation Loss: 28870133095537.777\n",
      "Epoch: 36\t Validation Loss: 28795208379050.668\n",
      "Epoch: 37\t Validation Loss: 28757420496213.332\n",
      "Epoch: 38\t Validation Loss: 28706206180238.223\n",
      "Epoch: 39\t Validation Loss: 28623121445319.11\n",
      "Epoch: 40\t Validation Loss: 28659228344320.0\n",
      "Epoch: 41\t Validation Loss: 28568509743104.0\n",
      "Epoch: 42\t Validation Loss: 28547817377336.89\n",
      "Epoch: 43\t Validation Loss: 28473876691171.555\n",
      "Epoch: 44\t Validation Loss: 28406370067342.223\n",
      "Epoch: 45\t Validation Loss: 28367535472640.0\n",
      "Epoch: 46\t Validation Loss: 28273640152177.777\n",
      "Epoch: 47\t Validation Loss: 28323700801536.0\n",
      "Epoch: 48\t Validation Loss: 28086824453916.445\n",
      "Epoch: 49\t Validation Loss: 28149244319061.332\n",
      "Epoch: 50\t Validation Loss: 28179150405632.0\n",
      "Epoch: 51\t Validation Loss: 28002839807772.445\n",
      "Epoch: 52\t Validation Loss: 28026322317767.11\n",
      "Epoch: 53\t Validation Loss: 27975814742016.0\n",
      "Epoch: 54\t Validation Loss: 27964927027882.668\n",
      "Epoch: 55\t Validation Loss: 27932569370624.0\n",
      "Epoch: 56\t Validation Loss: 27804679915292.445\n",
      "Epoch: 57\t Validation Loss: 27713481066268.445\n",
      "Epoch: 58\t Validation Loss: 27693564413724.445\n",
      "Epoch: 59\t Validation Loss: 27673378626673.777\n",
      "Epoch: 60\t Validation Loss: 27524016198997.332\n",
      "Epoch: 61\t Validation Loss: 27601519247360.0\n",
      "Epoch: 62\t Validation Loss: 27488369974385.777\n",
      "Epoch: 63\t Validation Loss: 27406200529806.223\n",
      "Epoch: 64\t Validation Loss: 27351178251832.89\n",
      "Epoch: 65\t Validation Loss: 27311933780423.11\n",
      "Epoch: 66\t Validation Loss: 27191425834097.777\n",
      "Epoch: 67\t Validation Loss: 27089865656092.445\n",
      "Epoch: 68\t Validation Loss: 27150013140536.89\n",
      "Epoch: 69\t Validation Loss: 27036555819235.555\n",
      "Epoch: 70\t Validation Loss: 26984104416597.332\n",
      "Epoch: 71\t Validation Loss: 26917537083847.11\n",
      "Epoch: 72\t Validation Loss: 26803786161265.777\n",
      "Epoch: 73\t Validation Loss: 26843987167004.445\n",
      "Epoch: 74\t Validation Loss: 26691580839708.445\n",
      "Epoch: 75\t Validation Loss: 27152087223864.89\n",
      "Epoch: 76\t Validation Loss: 26639166020721.777\n",
      "Epoch: 77\t Validation Loss: 26555503403918.223\n",
      "Epoch: 78\t Validation Loss: 26525610599310.223\n",
      "Epoch: 79\t Validation Loss: 26522262845667.555\n",
      "Epoch: 80\t Validation Loss: 26386094300273.777\n",
      "Epoch: 81\t Validation Loss: 26311820399957.332\n",
      "Epoch: 82\t Validation Loss: 26167295868928.0\n",
      "Epoch: 83\t Validation Loss: 26053054911829.332\n",
      "Epoch: 84\t Validation Loss: 26009005166136.89\n",
      "Epoch: 85\t Validation Loss: 26060383526001.777\n",
      "Epoch: 86\t Validation Loss: 25826227184526.223\n",
      "Epoch: 87\t Validation Loss: 25764418289664.0\n",
      "Epoch: 88\t Validation Loss: 25845718115214.223\n",
      "Epoch: 89\t Validation Loss: 25706571147946.668\n",
      "Epoch: 90\t Validation Loss: 25753898276181.332\n",
      "Epoch: 91\t Validation Loss: 25517832827335.11\n",
      "Epoch: 92\t Validation Loss: 25207146283008.0\n",
      "Epoch: 93\t Validation Loss: 25352644591616.0\n",
      "Epoch: 94\t Validation Loss: 25413161873863.11\n",
      "Epoch: 95\t Validation Loss: 25127921239836.445\n",
      "Epoch: 96\t Validation Loss: 25215326806926.223\n",
      "Epoch: 97\t Validation Loss: 25103875295004.445\n",
      "Epoch: 98\t Validation Loss: 24888398286392.89\n",
      "Epoch: 99\t Validation Loss: 25047601949354.668\n",
      "Epoch: 100\t Validation Loss: 24891843441095.11\n",
      "Epoch: 101\t Validation Loss: 24920671125504.0\n",
      "Epoch: 102\t Validation Loss: 24752461359786.668\n",
      "Epoch: 103\t Validation Loss: 24559909950805.332\n",
      "Epoch: 104\t Validation Loss: 24677822788039.11\n",
      "Epoch: 105\t Validation Loss: 24521465426375.11\n",
      "Epoch: 106\t Validation Loss: 24371855301745.777\n",
      "Epoch: 107\t Validation Loss: 24424443019264.0\n",
      "Epoch: 108\t Validation Loss: 24174077556963.555\n",
      "Epoch: 109\t Validation Loss: 24108178283633.777\n",
      "Epoch: 110\t Validation Loss: 24150671243491.555\n",
      "Epoch: 111\t Validation Loss: 24029198840263.11\n",
      "Epoch: 112\t Validation Loss: 23840407158784.0\n",
      "Epoch: 113\t Validation Loss: 23856669407459.555\n",
      "Epoch: 114\t Validation Loss: 23638285979192.89\n",
      "Epoch: 115\t Validation Loss: 23697719266872.89\n",
      "Epoch: 116\t Validation Loss: 23551088962218.668\n",
      "Epoch: 117\t Validation Loss: 23535330263040.0\n",
      "Epoch: 118\t Validation Loss: 23394971840967.11\n",
      "Epoch: 119\t Validation Loss: 23269983678919.11\n",
      "Epoch: 120\t Validation Loss: 23012268164892.445\n",
      "Epoch: 121\t Validation Loss: 23300930651932.445\n",
      "Epoch: 122\t Validation Loss: 22967660343751.11\n",
      "Epoch: 123\t Validation Loss: 22900401416874.668\n",
      "Epoch: 124\t Validation Loss: 22856109100600.89\n",
      "Epoch: 125\t Validation Loss: 22742846115384.89\n",
      "Epoch: 126\t Validation Loss: 22756810817536.0\n",
      "Epoch: 127\t Validation Loss: 22397739714332.445\n",
      "Epoch: 128\t Validation Loss: 22626639660828.445\n",
      "Epoch: 129\t Validation Loss: 22485306063075.555\n",
      "Epoch: 130\t Validation Loss: 22352538865208.89\n",
      "Epoch: 131\t Validation Loss: 22245373698958.223\n",
      "Epoch: 132\t Validation Loss: 22021485945742.223\n",
      "Epoch: 133\t Validation Loss: 21937688645176.89\n",
      "Epoch: 134\t Validation Loss: 21982218638677.332\n",
      "Epoch: 135\t Validation Loss: 21713787376981.332\n",
      "Epoch: 136\t Validation Loss: 21829757086378.668\n",
      "Epoch: 137\t Validation Loss: 21687790148721.777\n",
      "Epoch: 138\t Validation Loss: 21551150443178.668\n",
      "Epoch: 139\t Validation Loss: 21634409008696.89\n",
      "Epoch: 140\t Validation Loss: 21458660214556.445\n",
      "Epoch: 141\t Validation Loss: 21290814188202.668\n",
      "Epoch: 142\t Validation Loss: 21117470827406.223\n",
      "Epoch: 143\t Validation Loss: 21172884806769.777\n",
      "Epoch: 144\t Validation Loss: 20816541399267.555\n",
      "Epoch: 145\t Validation Loss: 20720698523648.0\n",
      "Epoch: 146\t Validation Loss: 20765270227171.555\n",
      "Epoch: 147\t Validation Loss: 20567437024369.777\n",
      "Epoch: 148\t Validation Loss: 20661117037226.668\n",
      "Epoch: 149\t Validation Loss: 20668375280298.668\n",
      "Epoch: 150\t Validation Loss: 19917676187192.89\n",
      "Epoch: 151\t Validation Loss: 20219232896796.445\n",
      "Epoch: 152\t Validation Loss: 20218608877568.0\n",
      "Epoch: 153\t Validation Loss: 20233642428188.445\n",
      "Epoch: 154\t Validation Loss: 20088779789653.332\n",
      "Epoch: 155\t Validation Loss: 19936087783651.555\n",
      "Epoch: 156\t Validation Loss: 19799549673472.0\n",
      "Epoch: 157\t Validation Loss: 19706576866872.89\n",
      "Epoch: 158\t Validation Loss: 19655547333290.668\n",
      "Epoch: 159\t Validation Loss: 19642671402552.89\n",
      "Epoch: 160\t Validation Loss: 19649392425187.555\n",
      "Epoch: 161\t Validation Loss: 19390906790343.11\n",
      "Epoch: 162\t Validation Loss: 19426861529315.555\n",
      "Epoch: 163\t Validation Loss: 19209037341582.223\n",
      "Epoch: 164\t Validation Loss: 19442640966997.332\n",
      "Epoch: 165\t Validation Loss: 18934489386552.89\n",
      "Epoch: 166\t Validation Loss: 18820305868117.332\n",
      "Epoch: 167\t Validation Loss: 18750399636366.223\n",
      "Epoch: 168\t Validation Loss: 18700680240241.777\n",
      "Epoch: 169\t Validation Loss: 18461893154133.332\n",
      "Epoch: 170\t Validation Loss: 18326663433784.89\n",
      "Epoch: 171\t Validation Loss: 18398821540750.223\n",
      "Epoch: 172\t Validation Loss: 18401350706062.223\n",
      "Epoch: 173\t Validation Loss: 18583936914773.332\n",
      "Epoch: 174\t Validation Loss: 18037421084216.89\n",
      "Epoch: 175\t Validation Loss: 18848198106225.777\n",
      "Epoch: 176\t Validation Loss: 17577920983495.111\n",
      "Epoch: 177\t Validation Loss: 17719752654848.0\n",
      "Epoch: 178\t Validation Loss: 17505221228771.555\n",
      "Epoch: 179\t Validation Loss: 17666735254186.668\n",
      "Epoch: 180\t Validation Loss: 17676309219100.445\n",
      "Epoch: 181\t Validation Loss: 17469898198584.889\n",
      "Epoch: 182\t Validation Loss: 17261515039630.223\n",
      "Epoch: 183\t Validation Loss: 17147509895623.111\n",
      "Epoch: 184\t Validation Loss: 17181909130353.777\n",
      "Epoch: 185\t Validation Loss: 17058735470819.555\n",
      "Epoch: 186\t Validation Loss: 16671828869120.0\n",
      "Epoch: 187\t Validation Loss: 16685011450083.555\n",
      "Epoch: 188\t Validation Loss: 16677107284195.555\n",
      "Epoch: 189\t Validation Loss: 16513780037859.555\n",
      "Epoch: 190\t Validation Loss: 16275545667811.555\n",
      "Epoch: 191\t Validation Loss: 16560838031587.555\n",
      "Epoch: 192\t Validation Loss: 16220419580814.223\n",
      "Epoch: 193\t Validation Loss: 15570032627256.889\n",
      "Epoch: 194\t Validation Loss: 15859994882503.111\n",
      "Epoch: 195\t Validation Loss: 15973127727786.666\n",
      "Epoch: 196\t Validation Loss: 15886612518229.334\n",
      "Epoch: 197\t Validation Loss: 15665167947093.334\n",
      "Epoch: 198\t Validation Loss: 15582623461831.111\n",
      "Epoch: 199\t Validation Loss: 15656903886620.445\n",
      "Epoch: 200\t Validation Loss: 15578694680576.0\n",
      "Epoch: 201\t Validation Loss: 15387211071488.0\n",
      "Epoch: 202\t Validation Loss: 14997081729706.666\n",
      "Epoch: 203\t Validation Loss: 15135529529799.111\n",
      "Epoch: 204\t Validation Loss: 14872847493802.666\n",
      "Epoch: 205\t Validation Loss: 14820110295950.223\n",
      "Epoch: 206\t Validation Loss: 15162760349468.445\n",
      "Epoch: 207\t Validation Loss: 15040367762545.777\n",
      "Epoch: 208\t Validation Loss: 14712718637283.555\n",
      "Epoch: 209\t Validation Loss: 14414179030357.334\n",
      "Epoch: 210\t Validation Loss: 14262871144675.555\n",
      "Epoch: 211\t Validation Loss: 14528937284949.334\n",
      "Epoch: 212\t Validation Loss: 13845223673400.889\n",
      "Epoch: 213\t Validation Loss: 14133384708096.0\n",
      "Epoch: 214\t Validation Loss: 13860658246087.111\n",
      "Epoch: 215\t Validation Loss: 13985960864426.666\n",
      "Epoch: 216\t Validation Loss: 13438251171840.0\n",
      "Epoch: 217\t Validation Loss: 13510026432056.889\n",
      "Epoch: 218\t Validation Loss: 13464650237724.445\n",
      "Epoch: 219\t Validation Loss: 13476885838051.555\n",
      "Epoch: 220\t Validation Loss: 13203035069553.777\n",
      "Epoch: 221\t Validation Loss: 13500121000618.666\n",
      "Epoch: 222\t Validation Loss: 13094401430869.334\n",
      "Epoch: 223\t Validation Loss: 12618549369969.777\n",
      "Epoch: 224\t Validation Loss: 12686070324337.777\n",
      "Epoch: 225\t Validation Loss: 13165255109290.666\n",
      "Epoch: 226\t Validation Loss: 12840356282368.0\n",
      "Epoch: 227\t Validation Loss: 12569088506083.555\n",
      "Epoch: 228\t Validation Loss: 12750796241123.555\n",
      "Epoch: 229\t Validation Loss: 12188159253617.777\n",
      "Epoch: 230\t Validation Loss: 12473025546012.445\n",
      "Epoch: 231\t Validation Loss: 11848859167402.666\n",
      "Epoch: 232\t Validation Loss: 11681943577486.223\n",
      "Epoch: 233\t Validation Loss: 12152490892401.777\n",
      "Epoch: 234\t Validation Loss: 11778468144014.223\n",
      "Epoch: 235\t Validation Loss: 11263150362168.889\n",
      "Epoch: 236\t Validation Loss: 11509539624277.334\n",
      "Epoch: 237\t Validation Loss: 11990329429560.889\n",
      "Epoch: 238\t Validation Loss: 12086455886734.223\n",
      "Epoch: 239\t Validation Loss: 10739555354396.445\n",
      "Epoch: 240\t Validation Loss: 11282583737685.334\n",
      "Epoch: 241\t Validation Loss: 11568526684615.111\n",
      "Epoch: 242\t Validation Loss: 11296527934350.223\n",
      "Epoch: 243\t Validation Loss: 10412447061333.334\n",
      "Epoch: 244\t Validation Loss: 10694699253760.0\n",
      "Epoch: 245\t Validation Loss: 9895865745408.0\n",
      "Epoch: 246\t Validation Loss: 10990190533745.777\n",
      "Epoch: 247\t Validation Loss: 10893030180636.445\n",
      "Epoch: 248\t Validation Loss: 11149025330972.445\n",
      "Epoch: 249\t Validation Loss: 10251497286314.666\n",
      "Epoch: 250\t Validation Loss: 9989460474083.555\n",
      "Epoch: 251\t Validation Loss: 10153795285447.111\n",
      "Epoch: 252\t Validation Loss: 10656172357859.555\n",
      "Epoch: 253\t Validation Loss: 9781974896184.889\n",
      "Epoch: 254\t Validation Loss: 9538257056199.111\n",
      "Epoch: 255\t Validation Loss: 9454890662115.555\n",
      "Epoch: 256\t Validation Loss: 9302901027271.111\n",
      "Epoch: 257\t Validation Loss: 9335552868352.0\n",
      "Epoch: 258\t Validation Loss: 9887141709596.445\n",
      "Epoch: 259\t Validation Loss: 9527988467939.555\n",
      "Epoch: 260\t Validation Loss: 8863966065095.111\n",
      "Epoch: 261\t Validation Loss: 9266440932920.889\n",
      "Epoch: 262\t Validation Loss: 9436317759715.555\n",
      "Epoch: 263\t Validation Loss: 8921159186659.555\n",
      "Epoch: 264\t Validation Loss: 9264353684138.666\n",
      "Epoch: 265\t Validation Loss: 8789977668721.777\n",
      "Epoch: 266\t Validation Loss: 8099262656967.111\n",
      "Epoch: 267\t Validation Loss: 8370473635384.889\n",
      "Epoch: 268\t Validation Loss: 8782023112931.556\n",
      "Epoch: 269\t Validation Loss: 8588066612110.223\n",
      "Epoch: 270\t Validation Loss: 8552037832021.333\n",
      "Epoch: 271\t Validation Loss: 8410516830435.556\n",
      "Epoch: 272\t Validation Loss: 8185988280775.111\n",
      "Epoch: 273\t Validation Loss: 7774295556096.0\n",
      "Epoch: 274\t Validation Loss: 8279259407246.223\n",
      "Epoch: 275\t Validation Loss: 8236740990293.333\n",
      "Epoch: 276\t Validation Loss: 7619367461774.223\n",
      "Epoch: 277\t Validation Loss: 7388078101845.333\n",
      "Epoch: 278\t Validation Loss: 7226349467875.556\n",
      "Epoch: 279\t Validation Loss: 7637917004231.111\n",
      "Epoch: 280\t Validation Loss: 7364999177102.223\n",
      "Epoch: 281\t Validation Loss: 7099064866133.333\n",
      "Epoch: 282\t Validation Loss: 7654754222080.0\n",
      "Epoch: 283\t Validation Loss: 7018988146232.889\n",
      "Epoch: 284\t Validation Loss: 6773644838684.444\n",
      "Epoch: 285\t Validation Loss: 6981200729429.333\n",
      "Epoch: 286\t Validation Loss: 6310881219925.333\n",
      "Epoch: 287\t Validation Loss: 6939135413816.889\n",
      "Epoch: 288\t Validation Loss: 6967679458417.777\n",
      "Epoch: 289\t Validation Loss: 7427620485347.556\n",
      "Epoch: 290\t Validation Loss: 6484319127324.444\n",
      "Epoch: 291\t Validation Loss: 5031965578581.333\n",
      "Epoch: 292\t Validation Loss: 6640718548536.889\n",
      "Epoch: 293\t Validation Loss: 6251514400312.889\n",
      "Epoch: 294\t Validation Loss: 5842255537038.223\n",
      "Epoch: 295\t Validation Loss: 6160072922453.333\n",
      "Epoch: 296\t Validation Loss: 6169630576184.889\n",
      "Epoch: 297\t Validation Loss: 5822746897066.667\n",
      "Epoch: 298\t Validation Loss: 6223540955818.667\n",
      "Epoch: 299\t Validation Loss: 5930447536128.0\n",
      "Epoch: 300\t Validation Loss: 5180823757710.223\n",
      "Epoch: 301\t Validation Loss: 5692666616945.777\n",
      "Epoch 00303: reducing learning rate of group 0 to 5.0000e-03.\n",
      "Epoch: 302\t Validation Loss: 5756798981461.333\n",
      "Epoch: 303\t Validation Loss: 5270147461575.111\n",
      "Epoch: 304\t Validation Loss: 5395579171271.111\n",
      "Epoch: 305\t Validation Loss: 5577593789553.777\n",
      "Epoch: 306\t Validation Loss: 5463933918321.777\n",
      "Early stopping\n",
      "--------------------------------\n",
      "Fold 3\n",
      "--------------------------------\n",
      "Epoch: 0\t Validation Loss: 29954163780266.668\n",
      "Epoch: 1\t Validation Loss: 29973094538353.777\n",
      "Epoch: 2\t Validation Loss: 29956015332465.777\n",
      "Epoch: 3\t Validation Loss: 29991981955299.555\n",
      "Epoch: 4\t Validation Loss: 29944542046890.668\n",
      "Epoch: 5\t Validation Loss: 29947809642723.555\n",
      "Epoch: 6\t Validation Loss: 29953550712832.0\n",
      "Epoch: 7\t Validation Loss: 29920315980003.555\n",
      "Epoch: 8\t Validation Loss: 29912165748280.89\n",
      "Epoch: 9\t Validation Loss: 29871465620366.223\n",
      "Epoch: 10\t Validation Loss: 29852621311544.89\n",
      "Epoch: 11\t Validation Loss: 29847884078193.777\n",
      "Epoch: 12\t Validation Loss: 29872126922296.89\n",
      "Epoch: 13\t Validation Loss: 29892763597027.555\n",
      "Epoch: 14\t Validation Loss: 29837902566741.332\n",
      "Epoch: 15\t Validation Loss: 29748596280888.89\n",
      "Epoch: 16\t Validation Loss: 29751989006791.11\n",
      "Epoch: 17\t Validation Loss: 29736076050432.0\n",
      "Epoch: 18\t Validation Loss: 29703161948842.668\n",
      "Epoch: 19\t Validation Loss: 29715160454485.332\n",
      "Epoch: 20\t Validation Loss: 29660592559445.332\n",
      "Epoch: 21\t Validation Loss: 29633850376192.0\n",
      "Epoch: 22\t Validation Loss: 29599832240583.11\n",
      "Epoch: 23\t Validation Loss: 29541866493269.332\n",
      "Epoch: 24\t Validation Loss: 29505448516721.777\n",
      "Epoch: 25\t Validation Loss: 29553811638044.445\n",
      "Epoch: 26\t Validation Loss: 29514094841400.89\n",
      "Epoch: 27\t Validation Loss: 29435924199651.555\n",
      "Epoch: 28\t Validation Loss: 29380992099214.223\n",
      "Epoch: 29\t Validation Loss: 29420884357575.11\n",
      "Epoch: 30\t Validation Loss: 29336204855068.445\n",
      "Epoch: 31\t Validation Loss: 29281694049166.223\n",
      "Epoch: 32\t Validation Loss: 29297456709632.0\n",
      "Epoch: 33\t Validation Loss: 29206930580366.223\n",
      "Epoch: 34\t Validation Loss: 29161763984725.332\n",
      "Epoch: 35\t Validation Loss: 29139955235043.555\n",
      "Epoch: 36\t Validation Loss: 29077046500465.777\n",
      "Epoch: 37\t Validation Loss: 29060198680348.445\n",
      "Epoch: 38\t Validation Loss: 28958177361920.0\n",
      "Epoch: 39\t Validation Loss: 28972741150492.445\n",
      "Epoch: 40\t Validation Loss: 28863049615132.445\n",
      "Epoch: 41\t Validation Loss: 28851096314766.223\n",
      "Epoch: 42\t Validation Loss: 28858022741788.445\n",
      "Epoch: 43\t Validation Loss: 28757840159630.223\n",
      "Epoch: 44\t Validation Loss: 28736532396259.555\n",
      "Epoch: 45\t Validation Loss: 28673663507569.777\n",
      "Epoch: 46\t Validation Loss: 28650459452757.332\n",
      "Epoch: 47\t Validation Loss: 28470727001884.445\n",
      "Epoch: 48\t Validation Loss: 28421357946652.445\n",
      "Epoch: 49\t Validation Loss: 28474542420423.11\n",
      "Epoch: 50\t Validation Loss: 28400608957781.332\n",
      "Epoch: 51\t Validation Loss: 28398521592490.668\n",
      "Epoch: 52\t Validation Loss: 28223160305436.445\n",
      "Epoch: 53\t Validation Loss: 28275031729038.223\n",
      "Epoch: 54\t Validation Loss: 28239004754830.223\n",
      "Epoch: 55\t Validation Loss: 28176668775765.332\n",
      "Epoch: 56\t Validation Loss: 28027003892167.11\n",
      "Epoch: 57\t Validation Loss: 27980390727680.0\n",
      "Epoch: 58\t Validation Loss: 27985010520519.11\n",
      "Epoch: 59\t Validation Loss: 27911947841991.11\n",
      "Epoch: 60\t Validation Loss: 27803936125383.11\n",
      "Epoch: 61\t Validation Loss: 27909044917589.332\n",
      "Epoch: 62\t Validation Loss: 27747253136042.668\n",
      "Epoch: 63\t Validation Loss: 27661370334321.777\n",
      "Epoch: 64\t Validation Loss: 27588061356942.223\n",
      "Epoch: 65\t Validation Loss: 27474490789432.89\n",
      "Epoch: 66\t Validation Loss: 27553593498737.777\n",
      "Epoch: 67\t Validation Loss: 27383693428508.445\n",
      "Epoch: 68\t Validation Loss: 27427266887680.0\n",
      "Epoch: 69\t Validation Loss: 27254668248860.445\n",
      "Epoch: 70\t Validation Loss: 27230417249166.223\n",
      "Epoch: 71\t Validation Loss: 27193989252892.445\n",
      "Epoch: 72\t Validation Loss: 27092639489137.777\n",
      "Epoch: 73\t Validation Loss: 27042336735232.0\n",
      "Epoch: 74\t Validation Loss: 26999138666268.445\n",
      "Epoch: 75\t Validation Loss: 26900012349667.555\n",
      "Epoch: 76\t Validation Loss: 26901900718535.11\n",
      "Epoch: 77\t Validation Loss: 26851898090382.223\n",
      "Epoch: 78\t Validation Loss: 26766622064640.0\n",
      "Epoch: 79\t Validation Loss: 26585450734478.223\n",
      "Epoch: 80\t Validation Loss: 26656226352241.777\n",
      "Epoch: 81\t Validation Loss: 26359111410574.223\n",
      "Epoch: 82\t Validation Loss: 26523030170282.668\n",
      "Epoch: 83\t Validation Loss: 26334642773105.777\n",
      "Epoch: 84\t Validation Loss: 26330278599793.777\n",
      "Epoch: 85\t Validation Loss: 26100292561578.668\n",
      "Epoch: 86\t Validation Loss: 26147111946012.445\n",
      "Epoch: 87\t Validation Loss: 25976224816241.777\n",
      "Epoch: 88\t Validation Loss: 25905533783608.89\n",
      "Epoch: 89\t Validation Loss: 25892611832035.555\n",
      "Epoch: 90\t Validation Loss: 25883199347825.777\n",
      "Epoch: 91\t Validation Loss: 25941350809600.0\n",
      "Epoch: 92\t Validation Loss: 25806559860053.332\n",
      "Epoch: 93\t Validation Loss: 25694137133738.668\n",
      "Epoch: 94\t Validation Loss: 25559781459740.445\n",
      "Epoch: 95\t Validation Loss: 25635512414663.11\n",
      "Epoch: 96\t Validation Loss: 25446299438648.89\n",
      "Epoch: 97\t Validation Loss: 25299876519025.777\n",
      "Epoch: 98\t Validation Loss: 25214319008881.777\n",
      "Epoch: 99\t Validation Loss: 25160873789212.445\n",
      "Epoch: 100\t Validation Loss: 25121420767687.11\n",
      "Epoch: 101\t Validation Loss: 25201047998008.89\n",
      "Epoch: 102\t Validation Loss: 25056501563392.0\n",
      "Epoch: 103\t Validation Loss: 24808058257408.0\n",
      "Epoch: 104\t Validation Loss: 24655697834439.11\n",
      "Epoch: 105\t Validation Loss: 24536986681344.0\n",
      "Epoch: 106\t Validation Loss: 24626496856974.223\n",
      "Epoch: 107\t Validation Loss: 24500173974186.668\n",
      "Epoch: 108\t Validation Loss: 24418673521095.11\n",
      "Epoch: 109\t Validation Loss: 24442997921109.332\n",
      "Epoch: 110\t Validation Loss: 24262623042673.777\n",
      "Epoch: 111\t Validation Loss: 24235966376618.668\n",
      "Epoch: 112\t Validation Loss: 24119598441358.223\n",
      "Epoch: 113\t Validation Loss: 24022242121045.332\n",
      "Epoch: 114\t Validation Loss: 23720523930737.777\n",
      "Epoch: 115\t Validation Loss: 23766506551523.555\n",
      "Epoch: 116\t Validation Loss: 23805463713109.332\n",
      "Epoch: 117\t Validation Loss: 23462361490318.223\n",
      "Epoch: 118\t Validation Loss: 23649872510976.0\n",
      "Epoch: 119\t Validation Loss: 23410431346460.445\n",
      "Epoch: 120\t Validation Loss: 23343979124053.332\n",
      "Epoch: 121\t Validation Loss: 23322740333681.777\n",
      "Epoch: 122\t Validation Loss: 23501094489656.89\n",
      "Epoch: 123\t Validation Loss: 23246892171264.0\n",
      "Epoch: 124\t Validation Loss: 23196618544469.332\n",
      "Epoch: 125\t Validation Loss: 22785417368917.332\n",
      "Epoch: 126\t Validation Loss: 23207493209656.89\n",
      "Epoch: 127\t Validation Loss: 22625946202567.11\n",
      "Epoch: 128\t Validation Loss: 22605020819911.11\n",
      "Epoch: 129\t Validation Loss: 22599760696661.332\n",
      "Epoch: 130\t Validation Loss: 22581986168376.89\n",
      "Epoch: 131\t Validation Loss: 22329636800284.445\n",
      "Epoch: 132\t Validation Loss: 22254701132003.555\n",
      "Epoch: 133\t Validation Loss: 22322574058382.223\n",
      "Epoch: 134\t Validation Loss: 22206304456248.89\n",
      "Epoch: 135\t Validation Loss: 22057858017962.668\n",
      "Epoch: 136\t Validation Loss: 22079186053802.668\n",
      "Epoch: 137\t Validation Loss: 21961731327772.445\n",
      "Epoch: 138\t Validation Loss: 21974570558350.223\n",
      "Epoch: 139\t Validation Loss: 21668086240597.332\n",
      "Epoch: 140\t Validation Loss: 21600958968263.11\n",
      "Epoch: 141\t Validation Loss: 21642926242019.555\n",
      "Epoch: 142\t Validation Loss: 21408304332800.0\n",
      "Epoch: 143\t Validation Loss: 21246681721514.668\n",
      "Epoch: 144\t Validation Loss: 21340748309390.223\n",
      "Epoch: 145\t Validation Loss: 21001299791416.89\n",
      "Epoch: 146\t Validation Loss: 21141064719473.777\n",
      "Epoch: 147\t Validation Loss: 20857731328682.668\n",
      "Epoch: 148\t Validation Loss: 20646872248775.11\n",
      "Epoch: 149\t Validation Loss: 20946688788252.445\n",
      "Epoch: 150\t Validation Loss: 20676950068792.89\n",
      "Epoch: 151\t Validation Loss: 20782791233080.89\n",
      "Epoch: 152\t Validation Loss: 20367045995178.668\n",
      "Epoch: 153\t Validation Loss: 20376439138986.668\n",
      "Epoch: 154\t Validation Loss: 20307990427875.555\n",
      "Epoch: 155\t Validation Loss: 20283807935146.668\n",
      "Epoch: 156\t Validation Loss: 20024030687687.11\n",
      "Epoch: 157\t Validation Loss: 19895260661532.445\n",
      "Epoch: 158\t Validation Loss: 20069663317105.777\n",
      "Epoch: 159\t Validation Loss: 19767318660892.445\n",
      "Epoch: 160\t Validation Loss: 19701953345763.555\n",
      "Epoch: 161\t Validation Loss: 19302516027847.11\n",
      "Epoch: 162\t Validation Loss: 19466363950421.332\n",
      "Epoch: 163\t Validation Loss: 19548655495850.668\n",
      "Epoch: 164\t Validation Loss: 19400774123520.0\n",
      "Epoch: 165\t Validation Loss: 19128016903281.777\n",
      "Epoch: 166\t Validation Loss: 18889631538289.777\n",
      "Epoch: 167\t Validation Loss: 18965169089194.668\n",
      "Epoch: 168\t Validation Loss: 18946093627619.555\n",
      "Epoch: 169\t Validation Loss: 18868737263388.445\n",
      "Epoch: 170\t Validation Loss: 18697578785450.668\n",
      "Epoch: 171\t Validation Loss: 18553311505066.668\n",
      "Epoch: 172\t Validation Loss: 18356227684579.555\n",
      "Epoch: 173\t Validation Loss: 18166519897201.777\n",
      "Epoch: 174\t Validation Loss: 18239876643043.555\n",
      "Epoch: 175\t Validation Loss: 18000914910776.89\n",
      "Epoch: 176\t Validation Loss: 18007801375402.668\n",
      "Epoch: 177\t Validation Loss: 17975638170737.777\n",
      "Epoch: 178\t Validation Loss: 17736573678023.11\n",
      "Epoch: 179\t Validation Loss: 17795873214008.89\n",
      "Epoch: 180\t Validation Loss: 17595440474794.668\n",
      "Epoch: 181\t Validation Loss: 17824325508209.777\n",
      "Epoch: 182\t Validation Loss: 17522436400014.223\n",
      "Epoch: 183\t Validation Loss: 17401214722958.223\n",
      "Epoch: 184\t Validation Loss: 17101272121344.0\n",
      "Epoch: 185\t Validation Loss: 17364274089528.889\n",
      "Epoch: 186\t Validation Loss: 17031507447352.889\n",
      "Epoch: 187\t Validation Loss: 17015392698368.0\n",
      "Epoch: 188\t Validation Loss: 16780038999608.889\n",
      "Epoch: 189\t Validation Loss: 16837644386304.0\n",
      "Epoch: 190\t Validation Loss: 16622701918435.555\n",
      "Epoch: 191\t Validation Loss: 16399301520042.666\n",
      "Epoch: 192\t Validation Loss: 16662833135616.0\n",
      "Epoch: 193\t Validation Loss: 16231969528945.777\n",
      "Epoch: 194\t Validation Loss: 16312597333788.445\n",
      "Epoch: 195\t Validation Loss: 16218091276060.445\n",
      "Epoch: 196\t Validation Loss: 15724996897450.666\n",
      "Epoch: 197\t Validation Loss: 15837307892167.111\n",
      "Epoch: 198\t Validation Loss: 15817137717248.0\n",
      "Epoch: 199\t Validation Loss: 15597792744789.334\n",
      "Epoch: 200\t Validation Loss: 15651267091569.777\n",
      "Epoch: 201\t Validation Loss: 15600543742179.555\n",
      "Epoch: 202\t Validation Loss: 15467632423367.111\n",
      "Epoch: 203\t Validation Loss: 15043599823303.111\n",
      "Epoch: 204\t Validation Loss: 15166450987463.111\n",
      "Epoch: 205\t Validation Loss: 14781598080113.777\n",
      "Epoch: 206\t Validation Loss: 15017711879964.445\n",
      "Epoch: 207\t Validation Loss: 14703329338254.223\n",
      "Epoch: 208\t Validation Loss: 14638937218161.777\n",
      "Epoch: 209\t Validation Loss: 14639145418752.0\n",
      "Epoch: 210\t Validation Loss: 14394216939520.0\n",
      "Epoch: 211\t Validation Loss: 14113431122375.111\n",
      "Epoch: 212\t Validation Loss: 14053765866382.223\n",
      "Epoch: 213\t Validation Loss: 14110525518279.111\n",
      "Epoch: 214\t Validation Loss: 13873140727808.0\n",
      "Epoch: 215\t Validation Loss: 14011360986908.445\n",
      "Epoch: 216\t Validation Loss: 13856824535722.666\n",
      "Epoch: 217\t Validation Loss: 13929199930936.889\n",
      "Epoch: 218\t Validation Loss: 13693396276565.334\n",
      "Epoch: 219\t Validation Loss: 13420327862272.0\n",
      "Epoch: 220\t Validation Loss: 13506446360576.0\n",
      "Epoch: 221\t Validation Loss: 13386028009244.445\n",
      "Epoch: 222\t Validation Loss: 13470745260487.111\n",
      "Epoch: 223\t Validation Loss: 13280205835832.889\n",
      "Epoch: 224\t Validation Loss: 12936486467811.555\n",
      "Epoch: 225\t Validation Loss: 12904136966144.0\n",
      "Epoch: 226\t Validation Loss: 12674442199040.0\n",
      "Epoch: 227\t Validation Loss: 13093028029326.223\n",
      "Epoch: 228\t Validation Loss: 12683581820472.889\n",
      "Epoch: 229\t Validation Loss: 12611720110990.223\n",
      "Epoch: 230\t Validation Loss: 12261265273287.111\n",
      "Epoch: 231\t Validation Loss: 12161309416561.777\n",
      "Epoch: 232\t Validation Loss: 12106287371605.334\n",
      "Epoch: 233\t Validation Loss: 12155970533603.555\n",
      "Epoch: 234\t Validation Loss: 11947263364664.889\n",
      "Epoch: 235\t Validation Loss: 12027593490432.0\n",
      "Epoch: 236\t Validation Loss: 11692322149717.334\n",
      "Epoch: 237\t Validation Loss: 11944253718528.0\n",
      "Epoch: 238\t Validation Loss: 11513411898936.889\n",
      "Epoch: 239\t Validation Loss: 11402656408462.223\n",
      "Epoch: 240\t Validation Loss: 11349973853070.223\n",
      "Epoch: 241\t Validation Loss: 11331541052074.666\n",
      "Epoch: 242\t Validation Loss: 11174888341504.0\n",
      "Epoch: 243\t Validation Loss: 11110329963861.334\n",
      "Epoch: 244\t Validation Loss: 10847933140536.889\n",
      "Epoch: 245\t Validation Loss: 10807627859740.445\n",
      "Epoch: 246\t Validation Loss: 10917339085027.555\n",
      "Epoch: 247\t Validation Loss: 10549085718300.445\n",
      "Epoch: 248\t Validation Loss: 10484836554069.334\n",
      "Epoch: 249\t Validation Loss: 10269476869461.334\n",
      "Epoch: 250\t Validation Loss: 10319346095445.334\n",
      "Epoch: 251\t Validation Loss: 10259932148167.111\n",
      "Epoch: 252\t Validation Loss: 9972585390990.223\n",
      "Epoch: 253\t Validation Loss: 10181640803669.334\n",
      "Epoch: 254\t Validation Loss: 9835336113265.777\n",
      "Epoch: 255\t Validation Loss: 9846587683271.111\n",
      "Epoch: 256\t Validation Loss: 9676163092024.889\n",
      "Epoch: 257\t Validation Loss: 9718152036352.0\n",
      "Epoch: 258\t Validation Loss: 9250607901354.666\n",
      "Epoch: 259\t Validation Loss: 9813650629518.223\n",
      "Epoch: 260\t Validation Loss: 9319994097664.0\n",
      "Epoch: 261\t Validation Loss: 9428793644373.334\n",
      "Epoch: 262\t Validation Loss: 9201055986119.111\n",
      "Epoch: 263\t Validation Loss: 9195597798513.777\n",
      "Epoch: 264\t Validation Loss: 9072459266275.555\n",
      "Epoch: 265\t Validation Loss: 9014305940366.223\n",
      "Epoch: 266\t Validation Loss: 8357827284536.889\n",
      "Epoch: 267\t Validation Loss: 8853619532686.223\n",
      "Epoch: 268\t Validation Loss: 8697028455082.667\n",
      "Epoch: 269\t Validation Loss: 8484684242033.777\n",
      "Epoch: 270\t Validation Loss: 8376998050019.556\n",
      "Epoch: 271\t Validation Loss: 8499120978147.556\n",
      "Epoch: 272\t Validation Loss: 8083513569735.111\n",
      "Epoch: 273\t Validation Loss: 8112985837112.889\n",
      "Epoch: 274\t Validation Loss: 7881725604295.111\n",
      "Epoch: 275\t Validation Loss: 7958129337230.223\n",
      "Epoch: 276\t Validation Loss: 7747789535459.556\n",
      "Epoch: 277\t Validation Loss: 7209962496910.223\n",
      "Epoch: 278\t Validation Loss: 7509607827228.444\n",
      "Epoch: 279\t Validation Loss: 7653731569208.889\n",
      "Epoch: 280\t Validation Loss: 7297600614855.111\n",
      "Epoch: 281\t Validation Loss: 7310080591644.444\n",
      "Epoch: 282\t Validation Loss: 7187400636643.556\n",
      "Epoch: 283\t Validation Loss: 7562107872142.223\n",
      "Epoch: 284\t Validation Loss: 7415443547022.223\n",
      "Epoch: 285\t Validation Loss: 7101996626375.111\n",
      "Epoch: 286\t Validation Loss: 6948001589930.667\n",
      "Epoch: 287\t Validation Loss: 7110780081493.333\n",
      "Epoch: 288\t Validation Loss: 7053706497592.889\n",
      "Epoch: 289\t Validation Loss: 6914230685240.889\n",
      "Epoch: 290\t Validation Loss: 6531199620437.333\n",
      "Epoch: 291\t Validation Loss: 6493818002545.777\n",
      "Epoch: 292\t Validation Loss: 6437914221681.777\n",
      "Epoch: 293\t Validation Loss: 6580322376817.777\n",
      "Epoch: 294\t Validation Loss: 6491437968042.667\n",
      "Epoch: 295\t Validation Loss: 6367445487160.889\n",
      "Epoch: 296\t Validation Loss: 6153098668714.667\n",
      "Epoch: 297\t Validation Loss: 6688174768128.0\n",
      "Epoch: 298\t Validation Loss: 6129022140416.0\n",
      "Epoch: 299\t Validation Loss: 5628815037781.333\n",
      "Epoch: 300\t Validation Loss: 5888417755591.111\n",
      "Epoch: 301\t Validation Loss: 5778286633870.223\n",
      "Epoch: 302\t Validation Loss: 5618587577002.667\n",
      "Epoch: 303\t Validation Loss: 5504292793002.667\n",
      "Epoch: 304\t Validation Loss: 5530944216177.777\n",
      "Epoch: 305\t Validation Loss: 5181332724849.777\n",
      "Epoch: 306\t Validation Loss: 5775325746517.333\n",
      "Epoch: 307\t Validation Loss: 4948818686407.111\n",
      "Epoch: 308\t Validation Loss: 5303772264675.556\n",
      "Epoch: 309\t Validation Loss: 5137194840519.111\n",
      "Epoch: 310\t Validation Loss: 5151887487431.111\n",
      "Epoch: 311\t Validation Loss: 5099975693653.333\n",
      "Epoch: 312\t Validation Loss: 4696899729635.556\n",
      "Epoch: 313\t Validation Loss: 4591081430129.777\n",
      "Epoch: 314\t Validation Loss: 4560116281799.111\n",
      "Epoch: 315\t Validation Loss: 4491920277504.0\n",
      "Epoch: 316\t Validation Loss: 4725500775992.889\n",
      "Epoch: 317\t Validation Loss: 4453908611072.0\n",
      "Epoch: 318\t Validation Loss: 4104758813582.222\n",
      "Epoch: 319\t Validation Loss: 4365340901376.0\n",
      "Epoch: 320\t Validation Loss: 4254681734257.778\n",
      "Epoch: 321\t Validation Loss: 3921029016234.6665\n",
      "Epoch: 322\t Validation Loss: 4239100972600.8887\n",
      "Epoch: 323\t Validation Loss: 4057484259783.1113\n",
      "Epoch: 324\t Validation Loss: 3928257054492.4443\n",
      "Epoch: 325\t Validation Loss: 3691732038087.1113\n",
      "Epoch: 326\t Validation Loss: 3918674147555.5557\n",
      "Epoch: 327\t Validation Loss: 3158946669454.222\n",
      "Epoch: 328\t Validation Loss: 3723055042104.8887\n",
      "Epoch: 329\t Validation Loss: 3786780413496.8887\n",
      "Epoch: 330\t Validation Loss: 3574732539676.4443\n",
      "Epoch: 331\t Validation Loss: 3630301853923.5557\n",
      "Epoch: 332\t Validation Loss: 3421705368007.1113\n",
      "Epoch: 333\t Validation Loss: 3399371427384.8887\n",
      "Epoch: 334\t Validation Loss: 2900408605809.778\n",
      "Epoch: 335\t Validation Loss: 2954134727338.6665\n",
      "Epoch: 336\t Validation Loss: 3129699146865.778\n",
      "Epoch: 337\t Validation Loss: 3016641915562.6665\n",
      "Epoch: 338\t Validation Loss: 3014748158179.5557\n",
      "Epoch: 339\t Validation Loss: 2668728185287.1113\n",
      "Epoch: 340\t Validation Loss: 3041991531633.778\n",
      "Epoch: 341\t Validation Loss: 2780312109056.0\n",
      "Epoch: 342\t Validation Loss: 3081696793941.3335\n",
      "Epoch: 343\t Validation Loss: 2593525334016.0\n",
      "Epoch: 344\t Validation Loss: 2905238696391.1113\n",
      "Epoch: 345\t Validation Loss: 2751111335480.8887\n",
      "Epoch: 346\t Validation Loss: 2699168841728.0\n",
      "Epoch: 347\t Validation Loss: 2195493938062.2222\n",
      "Epoch: 348\t Validation Loss: 2416643932160.0\n",
      "Epoch: 349\t Validation Loss: 2303921138346.6665\n",
      "Epoch: 350\t Validation Loss: 2345817545841.778\n",
      "Epoch: 351\t Validation Loss: 2073365388401.7778\n",
      "Epoch: 352\t Validation Loss: 2045708715804.4443\n",
      "Epoch: 353\t Validation Loss: 2259839542158.222\n",
      "Epoch: 354\t Validation Loss: 1896952365056.0\n",
      "Epoch: 355\t Validation Loss: 2146142796003.5557\n",
      "Epoch: 356\t Validation Loss: 1984245952967.111\n",
      "Epoch: 357\t Validation Loss: 1679251559765.3333\n",
      "Epoch: 358\t Validation Loss: 1983535047566.2222\n",
      "Epoch: 359\t Validation Loss: 2363720786830.222\n",
      "Epoch: 360\t Validation Loss: 1425806298680.889\n",
      "Epoch: 361\t Validation Loss: 2074861589845.3333\n",
      "Epoch: 362\t Validation Loss: 1988845487900.4443\n",
      "Epoch: 363\t Validation Loss: 2502210850360.8887\n",
      "Epoch: 364\t Validation Loss: 1843751041251.5557\n",
      "Epoch: 365\t Validation Loss: 1505273213383.111\n",
      "Epoch: 366\t Validation Loss: 1244156482901.3333\n",
      "Epoch: 367\t Validation Loss: 1296269078983.111\n",
      "Epoch: 368\t Validation Loss: 1558023401927.111\n",
      "Epoch: 369\t Validation Loss: 1860888690688.0\n",
      "Epoch: 370\t Validation Loss: 2253108092017.778\n",
      "Epoch: 371\t Validation Loss: 1120860562318.2222\n",
      "Epoch: 372\t Validation Loss: 1283714303772.4443\n",
      "Epoch: 373\t Validation Loss: 661644749482.6666\n",
      "Epoch: 374\t Validation Loss: 1341427533596.4443\n",
      "Epoch: 375\t Validation Loss: 1337517612145.7778\n",
      "Epoch: 376\t Validation Loss: 849752839509.3334\n",
      "Epoch: 377\t Validation Loss: 1673346882673.7778\n",
      "Epoch: 378\t Validation Loss: 1077331485582.2222\n",
      "Epoch: 379\t Validation Loss: 1282600570424.889\n",
      "Epoch: 380\t Validation Loss: 1089029086321.7778\n",
      "Epoch: 381\t Validation Loss: 1033286050702.2222\n",
      "Epoch: 382\t Validation Loss: 931368060700.4445\n",
      "Epoch: 383\t Validation Loss: 743868450588.4445\n",
      "Epoch: 384\t Validation Loss: 399489473649.7778\n",
      "Epoch: 385\t Validation Loss: 423091328341.3333\n",
      "Epoch: 386\t Validation Loss: 949540472604.4445\n",
      "Epoch: 387\t Validation Loss: 579286091548.4445\n",
      "Epoch: 388\t Validation Loss: 921208953969.7778\n",
      "Epoch: 389\t Validation Loss: 452876435456.0\n",
      "Epoch: 390\t Validation Loss: 540242345984.0\n",
      "Epoch: 391\t Validation Loss: 1413155287495.111\n",
      "Epoch: 392\t Validation Loss: 372375941575.1111\n",
      "Epoch: 393\t Validation Loss: 1171286523904.0\n",
      "Epoch: 394\t Validation Loss: 121792314481.77777\n",
      "Epoch: 395\t Validation Loss: 610240248035.5555\n",
      "Epoch: 396\t Validation Loss: 325810449066.6667\n",
      "Epoch: 397\t Validation Loss: 438563660686.2222\n",
      "Epoch: 398\t Validation Loss: 441889958570.6667\n",
      "Epoch: 399\t Validation Loss: 620904804807.1111\n",
      "Epoch: 400\t Validation Loss: 645151151445.3334\n",
      "Epoch: 401\t Validation Loss: 916956905472.0\n",
      "Epoch: 402\t Validation Loss: 323103235185.7778\n",
      "Epoch: 403\t Validation Loss: 298551873080.8889\n",
      "Epoch: 404\t Validation Loss: 154400785294.22223\n",
      "Epoch 00406: reducing learning rate of group 0 to 5.0000e-03.\n",
      "Epoch: 405\t Validation Loss: 537341019932.44446\n",
      "Epoch: 406\t Validation Loss: 500760015303.1111\n",
      "Epoch: 407\t Validation Loss: 474644662044.44446\n",
      "Epoch: 408\t Validation Loss: 337630738659.55554\n",
      "Epoch: 409\t Validation Loss: 243236725646.22223\n",
      "Early stopping\n",
      "--------------------------------\n",
      "Fold 4\n",
      "--------------------------------\n",
      "Epoch: 0\t Validation Loss: 29720307564544.0\n",
      "Epoch: 1\t Validation Loss: 29743077974926.223\n",
      "Epoch: 2\t Validation Loss: 29705717445063.11\n",
      "Epoch: 3\t Validation Loss: 29704081666503.11\n",
      "Epoch: 4\t Validation Loss: 29732084471125.332\n",
      "Epoch: 5\t Validation Loss: 29712386155406.223\n",
      "Epoch: 6\t Validation Loss: 29705291490190.223\n",
      "Epoch: 7\t Validation Loss: 29672090777827.555\n",
      "Epoch: 8\t Validation Loss: 29690763120184.89\n",
      "Epoch: 9\t Validation Loss: 29701451604878.223\n",
      "Epoch: 10\t Validation Loss: 29675505873351.11\n",
      "Epoch: 11\t Validation Loss: 29651175414897.777\n",
      "Epoch: 12\t Validation Loss: 29597667280668.445\n",
      "Epoch: 13\t Validation Loss: 29591586937969.777\n",
      "Epoch: 14\t Validation Loss: 29578070793329.777\n",
      "Epoch: 15\t Validation Loss: 29536671148714.668\n",
      "Epoch: 16\t Validation Loss: 29539993969550.223\n",
      "Epoch: 17\t Validation Loss: 29476207227335.11\n",
      "Epoch: 18\t Validation Loss: 29424295491811.555\n",
      "Epoch: 19\t Validation Loss: 29457097512277.332\n",
      "Epoch: 20\t Validation Loss: 29444464501646.223\n",
      "Epoch: 21\t Validation Loss: 29402964659768.89\n",
      "Epoch: 22\t Validation Loss: 29363880270961.777\n",
      "Epoch: 23\t Validation Loss: 29334768771982.223\n",
      "Epoch: 24\t Validation Loss: 29290297731754.668\n",
      "Epoch: 25\t Validation Loss: 29231080450730.668\n",
      "Epoch: 26\t Validation Loss: 29233853817742.223\n",
      "Epoch: 27\t Validation Loss: 29215594614328.89\n",
      "Epoch: 28\t Validation Loss: 29255062781952.0\n",
      "Epoch: 29\t Validation Loss: 29124316539562.668\n",
      "Epoch: 30\t Validation Loss: 29201675350471.11\n",
      "Epoch: 31\t Validation Loss: 29000498122296.89\n",
      "Epoch: 32\t Validation Loss: 29004139943253.332\n",
      "Epoch: 33\t Validation Loss: 28966157025280.0\n",
      "Epoch: 34\t Validation Loss: 28905289518648.89\n",
      "Epoch: 35\t Validation Loss: 28914270455580.445\n",
      "Epoch: 36\t Validation Loss: 28829027518236.445\n",
      "Epoch: 37\t Validation Loss: 28824696666339.555\n",
      "Epoch: 38\t Validation Loss: 28798895405283.555\n",
      "Epoch: 39\t Validation Loss: 28760774308295.11\n",
      "Epoch: 40\t Validation Loss: 28614427119160.89\n",
      "Epoch: 41\t Validation Loss: 28715045675918.223\n",
      "Epoch: 42\t Validation Loss: 28604567708558.223\n",
      "Epoch: 43\t Validation Loss: 28546209793820.445\n",
      "Epoch: 44\t Validation Loss: 28475709135985.777\n",
      "Epoch: 45\t Validation Loss: 28487884501447.11\n",
      "Epoch: 46\t Validation Loss: 28396144820224.0\n",
      "Epoch: 47\t Validation Loss: 28331941676828.445\n",
      "Epoch: 48\t Validation Loss: 28296693212046.223\n",
      "Epoch: 49\t Validation Loss: 28189775742748.445\n",
      "Epoch: 50\t Validation Loss: 28279983337927.11\n",
      "Epoch: 51\t Validation Loss: 28138778132480.0\n",
      "Epoch: 52\t Validation Loss: 28028979176334.223\n",
      "Epoch: 53\t Validation Loss: 28007394821916.445\n",
      "Epoch: 54\t Validation Loss: 27967520738872.89\n",
      "Epoch: 55\t Validation Loss: 27877888229376.0\n",
      "Epoch: 56\t Validation Loss: 27887953393891.555\n",
      "Epoch: 57\t Validation Loss: 27813129573717.332\n",
      "Epoch: 58\t Validation Loss: 27823476688668.445\n",
      "Epoch: 59\t Validation Loss: 27654511482197.332\n",
      "Epoch: 60\t Validation Loss: 27602182879459.555\n",
      "Epoch: 61\t Validation Loss: 27613177082311.11\n",
      "Epoch: 62\t Validation Loss: 27482418723043.555\n",
      "Epoch: 63\t Validation Loss: 27521217899178.668\n",
      "Epoch: 64\t Validation Loss: 27389055846172.445\n",
      "Epoch: 65\t Validation Loss: 27380838272568.89\n",
      "Epoch: 66\t Validation Loss: 27299183329280.0\n",
      "Epoch: 67\t Validation Loss: 27196250448782.223\n",
      "Epoch: 68\t Validation Loss: 27174751145528.89\n",
      "Epoch: 69\t Validation Loss: 27027308544000.0\n",
      "Epoch: 70\t Validation Loss: 26968372281344.0\n",
      "Epoch: 71\t Validation Loss: 26893610909696.0\n",
      "Epoch: 72\t Validation Loss: 26856724336184.89\n",
      "Epoch: 73\t Validation Loss: 26816016518712.89\n",
      "Epoch: 74\t Validation Loss: 26780034749781.332\n",
      "Epoch: 75\t Validation Loss: 26715692262286.223\n",
      "Epoch: 76\t Validation Loss: 26724874758826.668\n",
      "Epoch: 77\t Validation Loss: 26676336175786.668\n",
      "Epoch: 78\t Validation Loss: 26486554172529.777\n",
      "Epoch: 79\t Validation Loss: 26379923080988.445\n",
      "Epoch: 80\t Validation Loss: 26372672294456.89\n",
      "Epoch: 81\t Validation Loss: 26298167008369.777\n",
      "Epoch: 82\t Validation Loss: 26230488884110.223\n",
      "Epoch: 83\t Validation Loss: 26247405677226.668\n",
      "Epoch: 84\t Validation Loss: 26142652468792.89\n",
      "Epoch: 85\t Validation Loss: 26167141378730.668\n",
      "Epoch: 86\t Validation Loss: 25878434152448.0\n",
      "Epoch: 87\t Validation Loss: 25953838417692.445\n",
      "Epoch: 88\t Validation Loss: 25895858922382.223\n",
      "Epoch: 89\t Validation Loss: 25674291318328.89\n",
      "Epoch: 90\t Validation Loss: 25654948353365.332\n",
      "Epoch: 91\t Validation Loss: 25559824334848.0\n",
      "Epoch: 92\t Validation Loss: 25477401600910.223\n",
      "Epoch: 93\t Validation Loss: 25542895890887.11\n",
      "Epoch: 94\t Validation Loss: 25392085263246.223\n",
      "Epoch: 95\t Validation Loss: 25272904115086.223\n",
      "Epoch: 96\t Validation Loss: 25127706398264.89\n",
      "Epoch: 97\t Validation Loss: 25162765653333.332\n",
      "Epoch: 98\t Validation Loss: 25042094828202.668\n",
      "Epoch: 99\t Validation Loss: 24929365684679.11\n",
      "Epoch: 100\t Validation Loss: 24852213559751.11\n",
      "Epoch: 101\t Validation Loss: 24672151156963.555\n",
      "Epoch: 102\t Validation Loss: 24749121295701.332\n",
      "Epoch: 103\t Validation Loss: 24619684375210.668\n",
      "Epoch: 104\t Validation Loss: 24532259933752.89\n",
      "Epoch: 105\t Validation Loss: 24589681353614.223\n",
      "Epoch: 106\t Validation Loss: 24652551174371.555\n",
      "Epoch: 107\t Validation Loss: 24437012416284.445\n",
      "Epoch: 108\t Validation Loss: 24358909116416.0\n",
      "Epoch: 109\t Validation Loss: 24213816723228.445\n",
      "Epoch: 110\t Validation Loss: 24007661089223.11\n",
      "Epoch: 111\t Validation Loss: 24182639063495.11\n",
      "Epoch: 112\t Validation Loss: 23931032087210.668\n",
      "Epoch: 113\t Validation Loss: 23836473833699.555\n",
      "Epoch: 114\t Validation Loss: 23812808638464.0\n",
      "Epoch: 115\t Validation Loss: 23614494488803.555\n",
      "Epoch: 116\t Validation Loss: 23449260814791.11\n",
      "Epoch: 117\t Validation Loss: 23517117896021.332\n",
      "Epoch: 118\t Validation Loss: 23380916728263.11\n",
      "Epoch: 119\t Validation Loss: 23262442087310.223\n",
      "Epoch: 120\t Validation Loss: 23494204180252.445\n",
      "Epoch: 121\t Validation Loss: 23152423629255.11\n",
      "Epoch: 122\t Validation Loss: 23343978658019.555\n",
      "Epoch: 123\t Validation Loss: 23053894301923.555\n",
      "Epoch: 124\t Validation Loss: 22802762913109.332\n",
      "Epoch: 125\t Validation Loss: 22713547037809.777\n",
      "Epoch: 126\t Validation Loss: 22527174304654.223\n",
      "Epoch: 127\t Validation Loss: 22706413924807.11\n",
      "Epoch: 128\t Validation Loss: 22498094728988.445\n",
      "Epoch: 129\t Validation Loss: 22481103137450.668\n",
      "Epoch: 130\t Validation Loss: 22597076342101.332\n",
      "Epoch: 131\t Validation Loss: 21872603066823.11\n",
      "Epoch: 132\t Validation Loss: 22453521394346.668\n",
      "Epoch: 133\t Validation Loss: 22016214870698.668\n",
      "Epoch: 134\t Validation Loss: 22125529617749.332\n",
      "Epoch: 135\t Validation Loss: 21948532319118.223\n",
      "Epoch: 136\t Validation Loss: 21502095028679.11\n",
      "Epoch: 137\t Validation Loss: 21742924973852.445\n",
      "Epoch: 138\t Validation Loss: 21521156043207.11\n",
      "Epoch: 139\t Validation Loss: 21505681391616.0\n",
      "Epoch: 140\t Validation Loss: 21932854243783.11\n",
      "Epoch: 141\t Validation Loss: 21346059696355.555\n",
      "Epoch: 142\t Validation Loss: 20902471270400.0\n",
      "Epoch: 143\t Validation Loss: 21112416924103.11\n",
      "Epoch: 144\t Validation Loss: 20862315469937.777\n",
      "Epoch: 145\t Validation Loss: 21004583931448.89\n",
      "Epoch: 146\t Validation Loss: 20569734337877.332\n",
      "Epoch: 147\t Validation Loss: 21124276551680.0\n",
      "Epoch: 148\t Validation Loss: 20305273916984.89\n",
      "Epoch: 149\t Validation Loss: 20323428262798.223\n",
      "Epoch: 150\t Validation Loss: 20791115761436.445\n",
      "Epoch: 151\t Validation Loss: 20354152704682.668\n",
      "Epoch: 152\t Validation Loss: 19807386497479.11\n",
      "Epoch: 153\t Validation Loss: 20160363509987.555\n",
      "Epoch: 154\t Validation Loss: 19629006942663.11\n",
      "Epoch: 155\t Validation Loss: 20115353035662.223\n",
      "Epoch: 156\t Validation Loss: 20217525116017.777\n",
      "Epoch: 157\t Validation Loss: 19164081393208.89\n",
      "Epoch: 158\t Validation Loss: 19888796773034.668\n",
      "Epoch: 159\t Validation Loss: 19746766221767.11\n",
      "Epoch: 160\t Validation Loss: 19189376774599.11\n",
      "Epoch: 161\t Validation Loss: 18961607892081.777\n",
      "Epoch: 162\t Validation Loss: 18938945135502.223\n",
      "Epoch: 163\t Validation Loss: 19133204791296.0\n",
      "Epoch: 164\t Validation Loss: 18320750746737.777\n",
      "Epoch: 165\t Validation Loss: 18846738604942.223\n",
      "Epoch: 166\t Validation Loss: 18919262548423.11\n",
      "Epoch: 167\t Validation Loss: 18803358549788.445\n",
      "Epoch: 168\t Validation Loss: 18397108167566.223\n",
      "Epoch: 169\t Validation Loss: 18341595156024.89\n",
      "Epoch: 170\t Validation Loss: 18219047612871.11\n",
      "Epoch: 171\t Validation Loss: 18232314429440.0\n",
      "Epoch: 172\t Validation Loss: 18400671694848.0\n",
      "Epoch: 173\t Validation Loss: 17966511132216.89\n",
      "Epoch: 174\t Validation Loss: 18244667586787.555\n",
      "Epoch: 175\t Validation Loss: 17920206016056.89\n",
      "Epoch: 176\t Validation Loss: 17820448456704.0\n",
      "Epoch: 177\t Validation Loss: 17965154507889.777\n",
      "Epoch: 178\t Validation Loss: 17255801815040.0\n",
      "Epoch: 179\t Validation Loss: 17452410514090.666\n",
      "Epoch: 180\t Validation Loss: 17360595568412.445\n",
      "Epoch: 181\t Validation Loss: 17883906528597.332\n",
      "Epoch: 182\t Validation Loss: 17712426836878.223\n",
      "Epoch: 183\t Validation Loss: 16774545859470.223\n",
      "Epoch: 184\t Validation Loss: 16988776344234.666\n",
      "Epoch: 185\t Validation Loss: 17033127380764.445\n",
      "Epoch: 186\t Validation Loss: 16969589733603.555\n",
      "Epoch: 187\t Validation Loss: 16999081050112.0\n",
      "Epoch: 188\t Validation Loss: 16160084168248.889\n",
      "Epoch: 189\t Validation Loss: 16906541887943.111\n",
      "Epoch: 190\t Validation Loss: 15078226249500.445\n",
      "Epoch: 191\t Validation Loss: 15812168166058.666\n",
      "Epoch: 192\t Validation Loss: 16300697976832.0\n",
      "Epoch: 193\t Validation Loss: 16127879253560.889\n",
      "Epoch: 194\t Validation Loss: 16474229265749.334\n",
      "Epoch: 195\t Validation Loss: 15485544664632.889\n",
      "Epoch: 196\t Validation Loss: 15808939717063.111\n",
      "Epoch: 197\t Validation Loss: 15765223302030.223\n",
      "Epoch: 198\t Validation Loss: 15847113941902.223\n",
      "Epoch: 199\t Validation Loss: 15914341877532.445\n",
      "Epoch: 200\t Validation Loss: 15449224904704.0\n",
      "Epoch 00202: reducing learning rate of group 0 to 5.0000e-03.\n",
      "Epoch: 201\t Validation Loss: 15504450140387.555\n",
      "Epoch: 202\t Validation Loss: 15114103859882.666\n",
      "Epoch: 203\t Validation Loss: 15212098762979.555\n",
      "Epoch: 204\t Validation Loss: 15645682608810.666\n",
      "Epoch: 205\t Validation Loss: 14693571756032.0\n",
      "Epoch: 206\t Validation Loss: 15566202994688.0\n",
      "Epoch: 207\t Validation Loss: 14827213116757.334\n",
      "Epoch: 208\t Validation Loss: 15104117571584.0\n",
      "Epoch: 209\t Validation Loss: 14959416413752.889\n",
      "Epoch: 210\t Validation Loss: 14805221798343.111\n",
      "Epoch: 211\t Validation Loss: 14798657596074.666\n",
      "Epoch: 212\t Validation Loss: 14318033232782.223\n",
      "Epoch: 213\t Validation Loss: 15152978649998.223\n",
      "Epoch: 214\t Validation Loss: 14532071595121.777\n",
      "Epoch: 215\t Validation Loss: 14053751652352.0\n",
      "Epoch: 216\t Validation Loss: 14140158625564.445\n",
      "Epoch: 217\t Validation Loss: 15078328194389.334\n",
      "Epoch: 218\t Validation Loss: 14476720763335.111\n",
      "Epoch: 219\t Validation Loss: 14193865989233.777\n",
      "Epoch: 220\t Validation Loss: 13838511039374.223\n",
      "Epoch: 221\t Validation Loss: 14127645968156.445\n",
      "Epoch: 222\t Validation Loss: 14447639090517.334\n",
      "Epoch: 223\t Validation Loss: 14092494321891.555\n",
      "Epoch: 224\t Validation Loss: 14148420938410.666\n",
      "Epoch: 225\t Validation Loss: 15631989254826.666\n",
      "Epoch: 226\t Validation Loss: 13764443125987.555\n",
      "Epoch: 227\t Validation Loss: 13212250537984.0\n",
      "Epoch: 228\t Validation Loss: 13532076587235.555\n",
      "Epoch: 229\t Validation Loss: 14009530988771.555\n",
      "Epoch: 230\t Validation Loss: 13783544918471.111\n",
      "Epoch: 231\t Validation Loss: 13270401883249.777\n",
      "Epoch: 232\t Validation Loss: 13529349357568.0\n",
      "Epoch: 233\t Validation Loss: 13432260191118.223\n",
      "Epoch: 234\t Validation Loss: 13276626230385.777\n",
      "Epoch: 235\t Validation Loss: 13015799358350.223\n",
      "Epoch: 236\t Validation Loss: 13246133174272.0\n",
      "Epoch: 237\t Validation Loss: 12968130510848.0\n",
      "Epoch: 238\t Validation Loss: 13153833320448.0\n",
      "Epoch: 239\t Validation Loss: 13269103047111.111\n",
      "Epoch: 240\t Validation Loss: 12877113764522.666\n",
      "Epoch: 241\t Validation Loss: 12971793303324.445\n",
      "Epoch: 242\t Validation Loss: 12542825405553.777\n",
      "Epoch: 243\t Validation Loss: 12424033395598.223\n",
      "Epoch: 244\t Validation Loss: 12868741700721.777\n",
      "Epoch: 245\t Validation Loss: 13518722855367.111\n",
      "Epoch: 246\t Validation Loss: 12813558757603.555\n",
      "Epoch: 247\t Validation Loss: 12927060585130.666\n",
      "Epoch: 248\t Validation Loss: 12515570585144.889\n",
      "Epoch: 249\t Validation Loss: 12473365634161.777\n",
      "Epoch: 250\t Validation Loss: 12996564629731.555\n",
      "Epoch: 251\t Validation Loss: 12929893254940.445\n",
      "Epoch: 252\t Validation Loss: 12710471736433.777\n",
      "Epoch: 253\t Validation Loss: 12601968237681.777\n",
      "Epoch 00255: reducing learning rate of group 0 to 2.5000e-03.\n",
      "Epoch: 254\t Validation Loss: 12914678650197.334\n",
      "Epoch: 255\t Validation Loss: 12385537257927.111\n",
      "Epoch: 256\t Validation Loss: 12412980705507.555\n",
      "Epoch: 257\t Validation Loss: 12083320178460.445\n",
      "Epoch: 258\t Validation Loss: 12286603296768.0\n",
      "Epoch: 259\t Validation Loss: 11521099708643.555\n",
      "Epoch: 260\t Validation Loss: 12131687610595.555\n",
      "Epoch: 261\t Validation Loss: 12503798222392.889\n",
      "Epoch: 262\t Validation Loss: 12442189722055.111\n",
      "Epoch: 263\t Validation Loss: 12194757010318.223\n",
      "Epoch: 264\t Validation Loss: 11771457015352.889\n",
      "Epoch: 265\t Validation Loss: 11964887131022.223\n",
      "Epoch: 266\t Validation Loss: 12411669053440.0\n",
      "Epoch: 267\t Validation Loss: 11905009130609.777\n",
      "Epoch: 268\t Validation Loss: 12364075237376.0\n",
      "Epoch: 269\t Validation Loss: 12485811532231.111\n",
      "Epoch 00271: reducing learning rate of group 0 to 1.2500e-03.\n",
      "Epoch: 270\t Validation Loss: 12059517270243.555\n",
      "Epoch: 271\t Validation Loss: 11974065316750.223\n",
      "Epoch: 272\t Validation Loss: 11819178524672.0\n",
      "Epoch: 273\t Validation Loss: 11963897275278.223\n",
      "Epoch: 274\t Validation Loss: 11767411609144.889\n",
      "Early stopping\n",
      "--------------------------------\n",
      "Fold 5\n",
      "--------------------------------\n",
      "Epoch: 0\t Validation Loss: 29531068024604.445\n",
      "Epoch: 1\t Validation Loss: 29544413134848.0\n",
      "Epoch: 2\t Validation Loss: 29552140673934.223\n",
      "Epoch: 3\t Validation Loss: 29541785403392.0\n",
      "Epoch: 4\t Validation Loss: 29568629881059.555\n",
      "Epoch: 5\t Validation Loss: 29551482867256.89\n",
      "Epoch: 6\t Validation Loss: 29556659803477.332\n",
      "Epoch: 7\t Validation Loss: 29527549935616.0\n",
      "Epoch: 8\t Validation Loss: 29500684253411.555\n",
      "Epoch: 9\t Validation Loss: 29549824020024.89\n",
      "Epoch: 10\t Validation Loss: 29468310984021.332\n",
      "Epoch: 11\t Validation Loss: 29440941053269.332\n",
      "Epoch: 12\t Validation Loss: 29431383166520.89\n",
      "Epoch: 13\t Validation Loss: 29395678454670.223\n",
      "Epoch: 14\t Validation Loss: 29392542280362.668\n",
      "Epoch: 15\t Validation Loss: 29344273064846.223\n",
      "Epoch: 16\t Validation Loss: 29339669350172.445\n",
      "Epoch: 17\t Validation Loss: 29381223484984.89\n",
      "Epoch: 18\t Validation Loss: 29285785359701.332\n",
      "Epoch: 19\t Validation Loss: 29291620801649.777\n",
      "Epoch: 20\t Validation Loss: 29243227388131.555\n",
      "Epoch: 21\t Validation Loss: 29202277233095.11\n",
      "Epoch: 22\t Validation Loss: 29211250014435.555\n",
      "Epoch: 23\t Validation Loss: 29177811857863.11\n",
      "Epoch: 24\t Validation Loss: 29187212691228.445\n",
      "Epoch: 25\t Validation Loss: 29121598630570.668\n",
      "Epoch: 26\t Validation Loss: 29077392530545.777\n",
      "Epoch: 27\t Validation Loss: 29036141084672.0\n",
      "Epoch: 28\t Validation Loss: 29041097586915.555\n",
      "Epoch: 29\t Validation Loss: 28944878855054.223\n",
      "Epoch: 30\t Validation Loss: 28959635814627.555\n",
      "Epoch: 31\t Validation Loss: 28855896229660.445\n",
      "Epoch: 32\t Validation Loss: 28836061599061.332\n",
      "Epoch: 33\t Validation Loss: 28804808208839.11\n",
      "Epoch: 34\t Validation Loss: 28768474351388.445\n",
      "Epoch: 35\t Validation Loss: 28785861838620.445\n",
      "Epoch: 36\t Validation Loss: 28662216552903.11\n",
      "Epoch: 37\t Validation Loss: 28546819132984.89\n",
      "Epoch: 38\t Validation Loss: 28625205781390.223\n",
      "Epoch: 39\t Validation Loss: 28595493098837.332\n",
      "Epoch: 40\t Validation Loss: 28598686828316.445\n",
      "Epoch: 41\t Validation Loss: 28509139136967.11\n",
      "Epoch: 42\t Validation Loss: 28472114850474.668\n",
      "Epoch: 43\t Validation Loss: 28491102231665.777\n",
      "Epoch: 44\t Validation Loss: 28329625721969.777\n",
      "Epoch: 45\t Validation Loss: 28325226130090.668\n",
      "Epoch: 46\t Validation Loss: 28253115092536.89\n",
      "Epoch: 47\t Validation Loss: 28194148770702.223\n",
      "Epoch: 48\t Validation Loss: 28136489207580.445\n",
      "Epoch: 49\t Validation Loss: 28162064908288.0\n",
      "Epoch: 50\t Validation Loss: 28026264296561.777\n",
      "Epoch: 51\t Validation Loss: 28132996284416.0\n",
      "Epoch: 52\t Validation Loss: 28016823850325.332\n",
      "Epoch: 53\t Validation Loss: 27818832429056.0\n",
      "Epoch: 54\t Validation Loss: 27963524266211.555\n",
      "Epoch: 55\t Validation Loss: 27943545165141.332\n",
      "Epoch: 56\t Validation Loss: 27672011283569.777\n",
      "Epoch: 57\t Validation Loss: 27652873140451.555\n",
      "Epoch: 58\t Validation Loss: 27580274631566.223\n",
      "Epoch: 59\t Validation Loss: 27630128827960.89\n",
      "Epoch: 60\t Validation Loss: 27469316649415.11\n",
      "Epoch: 61\t Validation Loss: 27398500952746.668\n",
      "Epoch: 62\t Validation Loss: 27313324891249.777\n",
      "Epoch: 63\t Validation Loss: 27188323447239.11\n",
      "Epoch: 64\t Validation Loss: 27276741938744.89\n",
      "Epoch: 65\t Validation Loss: 27211887047111.11\n",
      "Epoch: 66\t Validation Loss: 27145394745799.11\n",
      "Epoch: 67\t Validation Loss: 27175970289891.555\n",
      "Epoch: 68\t Validation Loss: 27045119655936.0\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "class HousePricesModel(nn.Module):\n",
    "    def __init__(self,input_dim=22):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.head(x)\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def KFold_train_model(train_dataset,k_folds):\n",
    "    k_folds = k_folds\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "    loss_list = []  \n",
    "    val_loss_list = []\n",
    "    models = []\n",
    "\n",
    "\n",
    "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(train_dataset)):\n",
    "        print(f\"Fold {fold}\")\n",
    "        print('--------------------------------')\n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = SubsetRandomSampler(train_ids)\n",
    "        valid_subsampler = SubsetRandomSampler(valid_ids)\n",
    "\n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256,\n",
    "                                                    sampler=train_subsampler)\n",
    "        valid_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256,\n",
    "                                                    sampler=valid_subsampler)\n",
    "        \n",
    "        early_stopper = EarlyStopper(patience=15, min_delta=0.0001)\n",
    "\n",
    "        model = HousePricesModel()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.00001)\n",
    "        # Add reduce lr on plateau\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True, patience = 10, factor = 0.5)\n",
    "\n",
    "        loss_func = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(1000):\n",
    "            # Training\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                xb = xb.float()\n",
    "                yb = yb.float()\n",
    "                y_hat = model(xb)\n",
    "                y_hat = y_hat.squeeze()\n",
    "                loss = loss_func(y_hat,yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                valid_loss = 0\n",
    "                for xb, yb in valid_loader:\n",
    "                    xb = xb.float()\n",
    "                    yb = yb.float()\n",
    "                    y_hat = model(xb)\n",
    "                    y_hat = y_hat.squeeze()\n",
    "                    loss = loss_func(y_hat, yb)\n",
    "                    valid_loss += loss.item()\n",
    "\n",
    "                valid_loss /= len(valid_loader)\n",
    "                scheduler.step(valid_loss)\n",
    "\n",
    "            print(f\"Epoch: {epoch}\\t Validation Loss: {valid_loss}\")\n",
    "            if early_stopper.early_stop(valid_loss):\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "        val_loss_list.append(valid_loss)\n",
    "        models.append(model)\n",
    "        print('--------------------------------')\n",
    "\n",
    "    print('Average validation loss: ', np.mean(val_loss_list))\n",
    "    return models,val_loss_list\n",
    "\n",
    "def make_ensemble_predictions(models, test_loader):\n",
    "    \"\"\"Make ensemble predictions from the best models.\n",
    "    Average the predictions of the best models.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        model_predictions = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb in test_loader:\n",
    "                xb = xb[0]\n",
    "                y_hat = model(xb)\n",
    "                y_hat = y_hat.squeeze()\n",
    "                model_predictions.append(y_hat)\n",
    "        predictions.append(torch.cat(model_predictions))\n",
    "    predictions = torch.stack(predictions)\n",
    "    predictions = torch.mean(predictions, dim=0)\n",
    "    return predictions\n",
    "\n",
    "def save_predictions(preds, target=TARGET):\n",
    "    # Save predictions to csv file\n",
    "    df = pd.read_csv('sample_submission.csv')\n",
    "    df[target] = preds\n",
    "\n",
    "    # Make sure we have two columns in df\n",
    "    assert df.shape[1] == 2, f\"Expected df to have 2 columns, but got {df.shape[1]} columns\"\n",
    "    df.to_csv(\"submission.csv\", index=False)\n",
    "    return\n",
    "\n",
    "def submit_kaggle():\n",
    "    kaggle_cli = '/Users/dbless/Library/Python/3.11/bin/kaggle'\n",
    "    competition = COMPETITION\n",
    "    submission = \"submission.csv\"\n",
    "    message = \"Statistics may be dull, but it has its moments.\"\n",
    "    result = subprocess.run(['./submit_kaggle.sh',kaggle_cli,competition,submission,message],cwd=os.getcwd(), capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "    return\n",
    "models, val_loss_list = KFold_train_model(train_dataset,10)\n",
    "preds = make_ensemble_predictions(models, test_loader)\n",
    "save_predictions(preds)\n",
    "submit_kaggle()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "675111725fca00cf5c47bbec148c05600f90b048f52a44ff5ac679806e49bdda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Colab\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def check_colab():\n",
    "    \"\"\"Function to check if we are running on colab. Install packages if we are.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        IN_COLAB = True\n",
    "        print(\"Running on Colab\")\n",
    "        results = subprocess.run([\"pip\", \"install\", \"-r\", \"requirements.txt\"], check=True, capture_output=True)\n",
    "        # Check if the installation was successful\n",
    "        if results.returncode == 0:\n",
    "            print(\"Installation successful\")\n",
    "            print(\"You may need to restart the runtime for the changes to take effect\")\n",
    "        else:\n",
    "            print(\"Installation failed\")\n",
    "            print(results.stdout)\n",
    "        \n",
    "    except:\n",
    "        IN_COLAB = False\n",
    "        print(\"Not running on Colab\")\n",
    "    return IN_COLAB\n",
    "\n",
    "\n",
    "check_colab();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TARGET = 'MedHouseVal'\n",
    "COMPETITION = 'playground-series-s3e1'\n",
    "\n",
    "def train_val_split(input_tensors,targets, val_size=0.1, random_seed=0, true_random=False):\n",
    "    \"\"\"Splits the input tensors into train and validation sets. \n",
    "    Returns the train and validation sets as tensors.\n",
    "    \"\"\"\n",
    "    num_samples = input_tensors.shape[0]\n",
    "    indices = list(range(num_samples))\n",
    "    split = int(np.floor(val_size * num_samples))\n",
    "    if true_random:\n",
    "        random_seed = np.random.randint(0, 1000)\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    train_tensors = input_tensors[train_indices]\n",
    "    val_tensors = input_tensors[val_indices]\n",
    "    train_targets = targets[train_indices]\n",
    "    val_targets = targets[val_indices]\n",
    "\n",
    "    print(\"Train Size:\", train_tensors.shape)\n",
    "    print(\"Val Size:\", val_tensors.shape)\n",
    "\n",
    "    if train_tensors.shape[0] + val_tensors.shape[0] != input_tensors.shape[0]:\n",
    "        raise ValueError(\"Train and val sizes don't add up to input size\")\n",
    "\n",
    "    if train_tensors.shape[0] != train_targets.shape[0]:\n",
    "        raise ValueError(\"Train tensors and targets don't match\")\n",
    "\n",
    "    return train_tensors, val_tensors, train_targets, val_targets\n",
    "\n",
    "def load_data(target):\n",
    "    # Load the data, turn it into tensors\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "\n",
    "    print(\"Training Size:\", train_df.shape)\n",
    "    print(\"Test Size:\", test_df.shape)\n",
    "\n",
    "    FEATURES = [col for col in train_df.columns if col not in ['id', TARGET]]\n",
    "    print(\"Num Features:\", len(FEATURES))\n",
    "\n",
    "    train_tensors = torch.tensor(train_df[FEATURES].values, dtype=torch.float32)\n",
    "    target_tensors = torch.tensor(train_df[TARGET].values, dtype=torch.float32)\n",
    "    test_tensors = torch.tensor(test_df[FEATURES].values, dtype=torch.float32)\n",
    "    return train_tensors, target_tensors, test_tensors\n",
    "\n",
    "def get_device(whacky_mode = False):\n",
    "    # Get the device to train on\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available() and whacky_mode:\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def torch_standardize(x):\n",
    "    mean = torch.mean(x, dim=0)\n",
    "    std = torch.std(x, dim=0)\n",
    "    return ((x - mean) / std, mean, std)\n",
    "\n",
    "def plot_loss(**kwargs):\n",
    "    \"\"\"Plots the loss and accuracy of the model\"\"\"\n",
    "    # Get loss and accuracy from kwargs\n",
    "    loss_vals_ = kwargs.get('loss_vals', loss_vals)\n",
    "\n",
    "    # Get epochs to plot\n",
    "    total_epochs_shown = kwargs.get('epochs', 0)\n",
    "\n",
    "    loss_vals_ = loss_vals_[-total_epochs_shown:]\n",
    "    \n",
    "    plt.plot(loss_vals_, label=\"Loss\", color=\"green\", linestyle=\"dashed\", marker=\"o\")\n",
    "    plt.legend()\n",
    "    # Add title to x axis\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    # Add title to y axis\n",
    "    plt.ylabel(\"Loss\")\n",
    "    # Add title to graph\n",
    "    plt.title(\"Loss vs Epochs\")\n",
    "    # Only show integer epochs \n",
    "    # Set y-axis to log scale\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    # Plot accuracy in a seperate plot \n",
    "    plt.figure()\n",
    "    # Plot\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess(train_tensors, val_tensors, test_tensors, include_original=True):\n",
    "    \"\"\"Function for preprocessing the data before training. Currently normalizing,\n",
    "    standardizing, and keeping the original values for the neural network. We \n",
    "    will pass all three concatenated to our model, and let it decide which ones\n",
    "    to use.\n",
    "    We will L2, L1 normalize and min/max normalize the features of x.\n",
    "    For now we will focus on standardizing the data, and later we will add more\n",
    "    \"\"\"\n",
    "    original_train_tensors = train_tensors\n",
    "    original_val_tensors = val_tensors\n",
    "    original_test_tensors = test_tensors\n",
    "\n",
    "    train_tensors, train_mean, train_std = torch_standardize(train_tensors)\n",
    "    val_tensors = (val_tensors - train_mean) / train_std\n",
    "    test_tensors = (test_tensors - train_mean) / train_std\n",
    "\n",
    "    if include_original:\n",
    "        train_tensors = torch.cat((train_tensors, original_train_tensors), dim=1)\n",
    "        val_tensors = torch.cat((val_tensors, original_val_tensors), dim=1)\n",
    "        test_tensors = torch.cat((test_tensors, original_test_tensors), dim=1)\n",
    "    \n",
    "    return train_tensors, val_tensors, test_tensors\n",
    "\n",
    "def submit_kaggle(**kwargs):\n",
    "    \"\"\"Function for submitting a file to a kaggle competition. The function\n",
    "    will return the output of the kaggle cli command as a string.\"\"\"\n",
    "\n",
    "    kaggle_cli = kwargs.get(\"kaggle_cli\",'/Users/dbless/Library/Python/3.11/bin/kaggle')\n",
    "    competition = kwargs.get(\"competition\",\"playground-series-s3e1\")\n",
    "    submission = kwargs.get(\"submission\",\"submission.csv\")\n",
    "    message = kwargs.get(\"message\",\"Statistics may be dull, but it has its moments.\")\n",
    "\n",
    "    result = subprocess.run(['./submit_kaggle.sh',kaggle_cli,competition,submission,message],cwd=os.getcwd(), capture_output=True, text=True)\n",
    "\n",
    "    if hasattr(result, 'stderr'):\n",
    "        print(result.stderr)\n",
    "        \n",
    "    if result.returncode == 0:\n",
    "        print(\"Submission successful\")\n",
    "        \n",
    "    if hasattr(result, 'stdout'):\n",
    "        print(result.stdout)\n",
    "\n",
    "    return result\n",
    "\n",
    "class HousingDataset(Dataset):\n",
    "    \"\"\"Housing dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, x, y=None):\n",
    "        self.x = x\n",
    "        self.y = y if y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx] if self.y is not None else torch.empty((1, 1), dtype=torch.float32)\n",
    "\n",
    "def main_load(*args,**kwargs):\n",
    "\n",
    "    if kwargs.get(\"whacky_mode\",False):\n",
    "        print(\"Whacky mode activated\")\n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(X.values, dtype=torch.float32), torch.tensor(y.values, dtype=torch.float32))\n",
    "        test_dataset = TensorDataset(torch.tensor(X_test.values, dtype=torch.float32))\n",
    "        # Split train dataset\n",
    "        train_size = int(0.8 * len(train_dataset))\n",
    "        val_size = len(train_dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "        # Data loaders\n",
    "        # Get batch size if in kwargs\n",
    "        batch_size = kwargs.get('batch_size', 2500)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)      \n",
    "        print(\"Train dataloader batch size\", train_loader.batch_size)\n",
    "        # Wrap outputs in a dictionary\n",
    "        output = {\n",
    "            'train_tensors': X,\n",
    "            'val_tensors': y,\n",
    "            'test_tensors': X_test,\n",
    "            'train_dataset': train_dataset,\n",
    "            'val_dataset': val_dataset,\n",
    "            'test_dataset': test_dataset,\n",
    "            \"train_loader\": train_loader,\n",
    "            \"val_loader\": val_loader,\n",
    "            \"test_loader\": test_loader\n",
    "        }\n",
    "        return output\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    # Get target if in kwargs\n",
    "    target = kwargs.get('target', TARGET)\n",
    "    train_tensors, target_tensors, test_tensors = load_data(target)\n",
    "\n",
    "    # Get val_size, random seed and true random if in kwargs\n",
    "    val_size = kwargs.get('val_size', 0.2)\n",
    "    random_seed = kwargs.get('random_seed', 42)\n",
    "    true_random = kwargs.get('true_random', False)\n",
    "    train_tensors, val_tensors, train_targets, val_targets = train_val_split(train_tensors, target_tensors, val_size, random_seed, true_random)\n",
    "    include_original = kwargs.get('include_original', False)\n",
    "    train_tensors, val_tensors, test_tensors = preprocess(train_tensors, val_tensors, test_tensors,include_original=include_original)\n",
    "    train_dataset = TensorDataset(train_tensors, train_targets)\n",
    "    val_dataset = TensorDataset(val_tensors, val_targets)\n",
    "    test_dataset = TensorDataset(test_tensors)\n",
    "\n",
    "    # Data loaders\n",
    "    # Get batch size if in kwargs\n",
    "    batch_size = kwargs.get('batch_size', 2500)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"Train dataloader batch size\", train_loader.batch_size)\n",
    "\n",
    "    # Wrap outputs in a dictionary\n",
    "    output = {\n",
    "        'train_tensors': train_tensors,\n",
    "        'val_tensors': val_tensors,\n",
    "        'train_targets': train_targets,\n",
    "        'val_targets': val_targets,\n",
    "        'test_tensors': test_tensors,\n",
    "        'train_dataset': train_dataset,\n",
    "        'val_dataset': val_dataset,\n",
    "        'test_dataset': test_dataset,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"val_loader\": val_loader,\n",
    "        \"test_loader\": test_loader\n",
    "    }\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from umap import UMAP\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_df['r'] = np.sqrt(train_df['Latitude']**2 + train_df['Longitude']**2)\n",
    "train_df['theta'] = np.arctan2(train_df['Latitude'], train_df['Longitude'])\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_df['r'] = np.sqrt(test_df['Latitude']**2 + test_df['Longitude']**2)\n",
    "test_df['theta'] = np.arctan2(test_df['Latitude'], test_df['Longitude'])\n",
    "\n",
    "emb_size = 20\n",
    "precision = 1e6 \n",
    "df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "\n",
    "latlon = np.expand_dims(df[['Latitude', 'Longitude']].values, axis=-1) \n",
    "\n",
    "m = np.exp(np.log(precision) / emb_size) \n",
    "angle_freq = m ** np.arange(emb_size) \n",
    "angle_freq = angle_freq.reshape(1, 1, emb_size) \n",
    "\n",
    "latlon = latlon * angle_freq \n",
    "latlon[..., 0::2] = np.cos(latlon[..., 0::2]) \n",
    "latlon[..., 1::2] = np.sin(latlon[..., 1::2]) \n",
    "latlon = latlon.reshape(-1, 2 * emb_size) \n",
    "\n",
    "df['exp_latlon1'] = [lat[0] for lat in latlon]\n",
    "df['exp_latlon2'] = [lat[1] for lat in latlon]\n",
    "\n",
    "\n",
    "def pca(data):\n",
    "    '''\n",
    "    input: dataframe containing Latitude(x) and Longitude(y)\n",
    "    '''\n",
    "    coordinates = data[['Latitude','Latitude']].values\n",
    "    pca_obj = PCA().fit(coordinates)\n",
    "    pca_x = pca_obj.transform(data[['Latitude', 'Longitude']].values)[:,0]\n",
    "    pca_y = pca_obj.transform(data[['Latitude', 'Longitude']].values)[:,1]\n",
    "    return pca_x, pca_y\n",
    "\n",
    "# train_df['pca_x'], train_df['pca_y'] = pca(train_df)\n",
    "# test_df['pca_x'], test_df['pca_y'] = pca(test_df)\n",
    "df['pca_x'], df['pca_y'] = pca(df)\n",
    "def crt_crds(df): \n",
    "    df['rot_15_x'] = (np.cos(np.radians(15)) * df['Longitude']) + \\\n",
    "                      (np.sin(np.radians(15)) * df['Latitude'])\n",
    "    \n",
    "    df['rot_15_y'] = (np.cos(np.radians(15)) * df['Latitude']) + \\\n",
    "                      (np.sin(np.radians(15)) * df['Longitude'])\n",
    "    \n",
    "    df['rot_30_x'] = (np.cos(np.radians(30)) * df['Longitude']) + \\\n",
    "                      (np.sin(np.radians(30)) * df['Latitude'])\n",
    "    \n",
    "    df['rot_30_y'] = (np.cos(np.radians(30)) * df['Latitude']) + \\\n",
    "                      (np.sin(np.radians(30)) * df['Longitude'])\n",
    "    \n",
    "    df['rot_45_x'] = (np.cos(np.radians(45)) * df['Longitude']) + \\\n",
    "                      (np.sin(np.radians(45)) * df['Latitude'])\n",
    "    return df\n",
    "\n",
    "# train_df = crt_crds(train_df)\n",
    "# test_df = crt_crds(test_df)\n",
    "df = crt_crds(df)\n",
    "import reverse_geocoder as rg\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def geocoder(df):\n",
    "    coordinates = list(zip(df['Latitude'], df['Longitude']))\n",
    "    results = rg.search(coordinates)\n",
    "    return results\n",
    "\n",
    "# results = geocoder(train_df)\n",
    "# train_df['place'] = [x['admin2'] for x in results]\n",
    "# results = geocoder(test_df)\n",
    "# test_df['place'] = [x['admin2'] for x in results]\n",
    "\n",
    "results = geocoder(df)\n",
    "df['place'] = [x['admin2'] for x in results]\n",
    "\n",
    "places = ['Los Angeles County', 'Orange County', 'Kern County',\n",
    "          'Alameda County', 'San Francisco County', 'Ventura County',\n",
    "          'Santa Clara County', 'Fresno County', 'Santa Barbara County',\n",
    "          'Contra Costa County', 'Yolo County', 'Monterey County',\n",
    "          'Riverside County', 'Napa County']\n",
    "\n",
    "def replace(x):\n",
    "    if x in places:\n",
    "        return x\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "# train_df['place'] = train_df['place'].apply(lambda x: replace(x))\n",
    "# test_df['place'] = test_df['place'].apply(lambda x: replace(x))\n",
    "\n",
    "df['place'] = df['place'].apply(lambda x: replace(x))\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# train_df['place'] = le.fit_transform(train_df['place'])\n",
    "# test_df['place'] = le.transform(test_df['place'])\n",
    "\n",
    "# test_df = pd.get_dummies(test_df)\n",
    "# train_df = pd.get_dummies(train_df)\n",
    "\n",
    "df = pd.get_dummies(df)\n",
    "df\n",
    "from haversine import haversine\n",
    "\n",
    "Sac = (38.576931, -121.494949)\n",
    "SF = (37.780080, -122.420160)\n",
    "SJ = (37.334789, -121.888138)\n",
    "LA = (34.052235, -118.243683)\n",
    "SD = (32.715759, -117.163818)\n",
    "\n",
    "df['dist_Sac'] = df.apply(lambda x: haversine((x['Latitude'], x['Longitude']), Sac, unit='ft'), axis=1)\n",
    "df['dist_SF'] = df.apply(lambda x: haversine((x['Latitude'], x['Longitude']), SF, unit='ft'), axis=1)\n",
    "df['dist_SJ'] = df.apply(lambda x: haversine((x['Latitude'], x['Longitude']), SJ, unit='ft'), axis=1)\n",
    "df['dist_LA'] = df.apply(lambda x: haversine((x['Latitude'], x['Longitude']), LA, unit='ft'), axis=1)\n",
    "df['dist_SD'] = df.apply(lambda x: haversine((x['Latitude'], x['Longitude']), SD, unit='ft'), axis=1)\n",
    "df['dist_nearest_city'] = df[['dist_Sac', 'dist_SF', 'dist_SJ', \n",
    "                              'dist_LA', 'dist_SD']].min(axis=1)\n",
    "from shapely.geometry import LineString, Point\n",
    "\n",
    "coast_points = LineString([(32.6644, -117.1613), (33.2064, -117.3831),\n",
    "                           (33.7772, -118.2024), (34.4634, -120.0144),\n",
    "                           (35.4273, -120.8819), (35.9284, -121.4892),\n",
    "                           (36.9827, -122.0289), (37.6114, -122.4916),\n",
    "                           (38.3556, -123.0603), (39.7926, -123.8217),\n",
    "                           (40.7997, -124.1881), (41.7558, -124.1976)])\n",
    "\n",
    "df['dist_to_coast'] = df.apply(lambda x: Point(x['Latitude'], x['Longitude']).distance(coast_points), axis=1)\n",
    "# combine latitude and longitude\n",
    "# codes from \n",
    "# https://datascience.stackexchange.com/questions/49553/combining-latitude-longitude-position-into-single-feature\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def single_pt_haversine(lat, lng, degrees=True):\n",
    "    \"\"\"\n",
    "    'Single-point' Haversine: Calculates the great circle distance\n",
    "    between a point on Earth and the (0, 0) lat-long coordinate\n",
    "    \"\"\"\n",
    "    r = 6371 # Earth's radius (km). Have r = 3956 if you want miles\n",
    "\n",
    "    # Convert decimal degrees to radians\n",
    "    if degrees:\n",
    "        lat, lng = map(radians, [lat, lng])\n",
    "\n",
    "    # 'Single-point' Haversine formula\n",
    "    a = sin(lat/2)**2 + cos(lat) * sin(lng/2)**2\n",
    "    d = 2 * r * asin(sqrt(a)) \n",
    "\n",
    "    return d\n",
    "# add more metric \n",
    "# referred to this discussion\n",
    "# https://www.kaggle.com/competitions/playground-series-s3e1/discussion/376210\n",
    "\n",
    "def manhattan(lat,lng):\n",
    "    return np.abs(lat) + np.abs(lng)\n",
    "def euclidean(lat,lng):\n",
    "    return (lat**2 + lng**2) **0.5\n",
    "\n",
    "def add_combine(df):      \n",
    "    df['haversine'] = [single_pt_haversine(x, y) for x, y in zip(df.Latitude, df.Longitude)]\n",
    "    df['manhattan'] = [manhattan(x,y) for x,y in zip(df.Latitude, df.Longitude)]\n",
    "    df['euclidean'] = [euclidean(x,y) for x,y in zip(df.Latitude,df.Longitude)]\n",
    "    return df\n",
    "\n",
    "df = add_combine(df)\n",
    "df['number_houses_per_block'] = df['Population'] / df['AveOccup']\n",
    "df['total_income_of_block'] = df['MedInc'] * df['Population']\n",
    "df['occupants_to_bedrooms'] = df['AveOccup'] / df['AveBedrms']\n",
    "df['total_number_of_rooms'] = df['AveBedrms'] + df['AveRooms']\n",
    "df['bedrooms_to_rooms'] = df['AveBedrms'] / df['AveRooms']\n",
    "df['occupants_to_rooms'] = df['AveOccup'] / df['AveRooms']\n",
    "train_df = df.iloc[:-len(test_df),:]\n",
    "test_df = df.iloc[-len(test_df):,:].drop('MedHouseVal', axis=1).reset_index(drop=True)\n",
    "\n",
    "X = train_df.drop(['MedHouseVal', 'id'], axis=1)\n",
    "y = train_df.MedHouseVal\n",
    "X_test = test_df.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whacky mode activated\n",
      "Train dataloader batch size 2500\n",
      "------------------------------\n",
      "Training model: HousePricesModel\n",
      "Optimizer: Adam\n",
      "\n",
      "Epoch: 0\n",
      "Validation loss: 1883747.3333333333\n",
      "\n",
      "Epoch: 1\n",
      "Validation loss: 6482593.333333333\n",
      "\n",
      "Epoch: 2\n",
      "Validation loss: 566046.4583333334\n",
      "\n",
      "Epoch: 3\n",
      "Validation loss: 101689.7421875\n",
      "\n",
      "Epoch: 4\n",
      "Validation loss: 116101.6328125\n",
      "\n",
      "Epoch: 5\n",
      "Validation loss: 128595.66145833333\n",
      "\n",
      "Epoch: 6\n",
      "Validation loss: 149471.54166666666\n",
      "\n",
      "Epoch: 7\n",
      "Validation loss: 156472.453125\n",
      "\n",
      "Epoch: 8\n",
      "Validation loss: 48035.817708333336\n",
      "\n",
      "Epoch: 9\n",
      "Validation loss: 36201.2421875\n",
      "\n",
      "Epoch: 10\n",
      "Validation loss: 16737.3330078125\n",
      "\n",
      "Epoch: 11\n",
      "Validation loss: 4497.2529296875\n",
      "\n",
      "Epoch: 12\n",
      "Validation loss: 1126.2189127604167\n",
      "\n",
      "Epoch: 13\n",
      "Validation loss: 2450.98095703125\n",
      "\n",
      "Epoch: 14\n",
      "Validation loss: 1032.3584594726562\n",
      "\n",
      "Epoch: 15\n",
      "Validation loss: 572.4054768880209\n",
      "\n",
      "Epoch: 16\n",
      "Validation loss: 392.62005615234375\n",
      "\n",
      "Epoch: 17\n",
      "Validation loss: 251.44475301106772\n",
      "\n",
      "Epoch: 18\n",
      "Validation loss: 148.00963846842447\n",
      "\n",
      "Epoch: 19\n",
      "Validation loss: 47.78555170694987\n",
      "\n",
      "Epoch: 20\n",
      "Validation loss: 8.433846473693848\n",
      "\n",
      "Epoch: 21\n",
      "Validation loss: 5.1876271565755205\n",
      "\n",
      "Epoch: 22\n",
      "Validation loss: 5.187567392985026\n",
      "\n",
      "Epoch: 23\n",
      "Validation loss: 5.187498251597087\n",
      "\n",
      "Epoch: 24\n",
      "Validation loss: 5.187389850616455\n",
      "\n",
      "Epoch: 25\n",
      "Validation loss: 5.18726110458374\n",
      "\n",
      "Epoch: 26\n",
      "Validation loss: 5.187150637308757\n",
      "\n",
      "Epoch: 27\n",
      "Validation loss: 5.187010765075684\n",
      "\n",
      "Epoch: 28\n",
      "Validation loss: 5.1868851979573565\n",
      "\n",
      "Epoch: 29\n",
      "Validation loss: 5.186745007832845\n",
      "\n",
      "Epoch: 30\n",
      "Validation loss: 5.186641693115234\n",
      "\n",
      "Epoch: 31\n",
      "Validation loss: 5.186550458272298\n",
      "\n",
      "Epoch: 32\n",
      "Validation loss: 5.186450799306233\n",
      "\n",
      "Epoch: 33\n",
      "Validation loss: 5.186345895131429\n",
      "\n",
      "Epoch: 34\n",
      "Validation loss: 5.186215241750081\n",
      "\n",
      "Epoch: 35\n",
      "Validation loss: 5.186153570810954\n",
      "\n",
      "Epoch: 36\n",
      "Validation loss: 5.185989856719971\n",
      "\n",
      "Epoch: 37\n",
      "Validation loss: 5.185812473297119\n",
      "\n",
      "Epoch: 38\n",
      "Validation loss: 5.1856381098429365\n",
      "\n",
      "Epoch: 39\n",
      "Validation loss: 5.185502211252849\n",
      "\n",
      "Epoch: 40\n",
      "Validation loss: 5.185368696848552\n",
      "\n",
      "Epoch: 41\n",
      "Validation loss: 5.185255527496338\n",
      "\n",
      "Epoch: 42\n",
      "Validation loss: 5.185104052225749\n",
      "\n",
      "Epoch: 43\n",
      "Validation loss: 5.184935569763184\n",
      "\n",
      "Epoch: 44\n",
      "Validation loss: 5.184775034586589\n",
      "\n",
      "Epoch: 45\n",
      "Validation loss: 5.184644858042399\n",
      "\n",
      "Epoch: 46\n",
      "Validation loss: 5.184512615203857\n",
      "\n",
      "Epoch: 47\n",
      "Validation loss: 5.184387842814128\n",
      "\n",
      "Epoch: 48\n",
      "Validation loss: 5.184269269307454\n",
      "\n",
      "Epoch: 49\n",
      "Validation loss: 5.1841451327006025\n",
      "\n",
      "Epoch: 50\n",
      "Validation loss: 5.184070110321045\n",
      "\n",
      "Epoch: 51\n",
      "Validation loss: 5.183982054392497\n",
      "\n",
      "Epoch: 52\n",
      "Validation loss: 5.183846950531006\n",
      "\n",
      "Epoch: 53\n",
      "Validation loss: 5.183714230855306\n",
      "\n",
      "Epoch: 54\n",
      "Validation loss: 5.183601538340251\n",
      "\n",
      "Epoch: 55\n",
      "Validation loss: 5.183470408121745\n",
      "\n",
      "Epoch: 56\n",
      "Validation loss: 5.183351516723633\n",
      "\n",
      "Epoch: 57\n",
      "Validation loss: 5.18322229385376\n",
      "\n",
      "Epoch: 58\n",
      "Validation loss: 5.183091640472412\n",
      "\n",
      "Epoch: 59\n",
      "Validation loss: 5.182941595713298\n",
      "\n",
      "Epoch: 60\n",
      "Validation loss: 5.182841142018636\n",
      "\n",
      "Epoch: 61\n",
      "Validation loss: 5.182740370432536\n",
      "\n",
      "Epoch: 62\n",
      "Validation loss: 5.182631969451904\n",
      "\n",
      "Epoch: 63\n",
      "Validation loss: 5.182527383168538\n",
      "\n",
      "Epoch: 64\n",
      "Validation loss: 5.182419300079346\n",
      "\n",
      "Epoch: 65\n",
      "Validation loss: 5.182277679443359\n",
      "\n",
      "Epoch: 66\n",
      "Validation loss: 5.182188193003337\n",
      "\n",
      "Epoch: 67\n",
      "Validation loss: 5.182077407836914\n",
      "\n",
      "Epoch: 68\n",
      "Validation loss: 5.1819413503011065\n",
      "\n",
      "Epoch: 69\n",
      "Validation loss: 5.181854248046875\n",
      "\n",
      "Epoch: 70\n",
      "Validation loss: 5.181758085886638\n",
      "\n",
      "Epoch: 71\n",
      "Validation loss: 5.181660493214925\n",
      "\n",
      "Epoch: 72\n",
      "Validation loss: 5.18152395884196\n",
      "\n",
      "Epoch: 73\n",
      "Validation loss: 5.181375662485759\n",
      "\n",
      "Epoch: 74\n",
      "Validation loss: 5.1812136967976885\n",
      "\n",
      "Epoch: 75\n",
      "Validation loss: 5.181060314178467\n",
      "\n",
      "Epoch: 76\n",
      "Validation loss: 5.180922985076904\n",
      "\n",
      "Epoch: 77\n",
      "Validation loss: 5.180815060933431\n",
      "\n",
      "Epoch: 78\n",
      "Validation loss: 5.180691083272298\n",
      "\n",
      "Epoch: 79\n",
      "Validation loss: 5.1805799802144366\n",
      "\n",
      "Epoch: 80\n",
      "Validation loss: 5.180428822835286\n",
      "\n",
      "Epoch: 81\n",
      "Validation loss: 5.1803083419799805\n",
      "\n",
      "Epoch: 82\n",
      "Validation loss: 5.180187225341797\n",
      "\n",
      "Epoch: 83\n",
      "Validation loss: 5.180066744486491\n",
      "\n",
      "Epoch: 84\n",
      "Validation loss: 5.179947376251221\n",
      "\n",
      "Epoch: 85\n",
      "Validation loss: 5.179818471272786\n",
      "\n",
      "Epoch: 86\n",
      "Validation loss: 5.179684003194173\n",
      "\n",
      "Epoch: 87\n",
      "Validation loss: 5.179567019144694\n",
      "\n",
      "Epoch: 88\n",
      "Validation loss: 5.179437478383382\n",
      "\n",
      "Epoch: 89\n",
      "Validation loss: 5.179291566212972\n",
      "\n",
      "Epoch: 90\n",
      "Validation loss: 5.17917013168335\n",
      "\n",
      "Epoch: 91\n",
      "Validation loss: 5.179059028625488\n",
      "\n",
      "Epoch: 92\n",
      "Validation loss: 5.178971131642659\n",
      "\n",
      "Epoch: 93\n",
      "Validation loss: 5.1788655916849775\n",
      "\n",
      "Epoch: 94\n",
      "Validation loss: 5.178747812906901\n",
      "\n",
      "Epoch: 95\n",
      "Validation loss: 5.178604920705159\n",
      "\n",
      "Epoch: 96\n",
      "Validation loss: 5.178428967793782\n",
      "\n",
      "Epoch: 97\n",
      "Validation loss: 5.178295453389485\n",
      "\n",
      "Epoch: 98\n",
      "Validation loss: 5.178171952565511\n",
      "\n",
      "Epoch: 99\n",
      "Validation loss: 5.178047180175781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class HousePricesModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HousePricesModel, self).__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(50, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(x)\n",
    "    \n",
    "def train(model, train_dl, val_dl, optimizer, loss_func, epochs, device):\n",
    "    loss_vals = []\n",
    "    print(\"-\"*30)\n",
    "    print(f\"Training model: {model.__class__.__name__}\")\n",
    "    print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "    print()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            y_hat = model(xb)\n",
    "            y_hat = y_hat.squeeze()\n",
    "            loss = loss_func(y_hat, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss = 0\n",
    "            for xb, yb in val_dl:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                \n",
    "                y_hat = model(xb)\n",
    "                y_hat = y_hat.squeeze()\n",
    "                loss = loss_func(y_hat, yb)\n",
    "                tot_loss += loss.item() \n",
    "            val_loss = tot_loss / len(val_dl)\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            print(f\"Validation loss: {val_loss}\")\n",
    "            print()\n",
    "            loss_vals.append(val_loss)\n",
    "\n",
    "    return loss_vals\n",
    "\n",
    "def train_model(*args, **kwargs):\n",
    "    \"\"\"Function for training a model. We will use this function to train the\n",
    "    model with the optimal hyperparameters found in the hyperparameter search.\"\"\"\n",
    "\n",
    "    # Get model, optimizer, loss function and dataloaders from kwargs, or use default values\n",
    "    device = kwargs.get('dev', get_device())\n",
    "    model = kwargs.get('model',HousePricesModel()).to(device)\n",
    "    lr = kwargs.get('lr', 0.001)\n",
    "    weight_decay = kwargs.get('weight_decay', 0)\n",
    "    optimizer = kwargs.get('optimizer',optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay))\n",
    "    loss_func = kwargs.get('loss_func', nn.MSELoss(reduction='mean'))\n",
    "    train_dl = kwargs.get('train_dl',data_dict['train_loader'])\n",
    "    val_dl = kwargs.get('val_dl',data_dict['val_loader'])\n",
    "    epochs = kwargs.get('epochs', 100)\n",
    "\n",
    "    # Train model\n",
    "    input_model_dict = dict(model=model, optimizer=optimizer, loss_func=loss_func, train_dl=train_dl, val_dl=val_dl, epochs=epochs, device=device)\n",
    "\n",
    "    return train(**input_model_dict),model\n",
    "\n",
    "# Get model predictions\n",
    "def make_predictions(model,**kwargs):\n",
    "    device = kwargs.get('device', get_device())\n",
    "    test_dl = kwargs.get('test_dl', data_dict['test_loader'])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tot_loss = 0\n",
    "        preds = []\n",
    "        for xb in test_dl:\n",
    "            xb = xb[0]\n",
    "            xb = xb.to(device)\n",
    "            y_hat = model(xb)\n",
    "            y_hat = y_hat.squeeze()\n",
    "            preds.append(y_hat)\n",
    "        preds = torch.cat(preds)\n",
    "        preds = preds.cpu().numpy()\n",
    "\n",
    "    # Check preds are correct shape\n",
    "    correct_num_of_preds = kwargs.get('correct_num_of_preds', data_dict['test_tensors'].shape[0])\n",
    "    if preds.shape[0] != correct_num_of_preds:\n",
    "        raise ValueError(f\"Expected {correct_num_of_preds} predictions, but got {preds.shape[0]} predictions\")\n",
    "    return preds\n",
    "\n",
    "def save_predictions(preds, **kwargs):\n",
    "    # Save predictions to csv file\n",
    "    filename = kwargs.get('filename', 'submission.csv')\n",
    "    target = kwargs.get('target', TARGET)\n",
    "    # Copy Ids from sample submission\n",
    "    df = pd.read_csv('sample_submission.csv')\n",
    "    df[target] = preds\n",
    "\n",
    "    # Make sure we have two columns in df\n",
    "    assert df.shape[1] == 2, f\"Expected df to have 2 columns, but got {df.shape[1]} columns\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    return\n",
    "\n",
    "def make_and_save_predictions(model, **kwargs):\n",
    "    preds = make_predictions(model, **kwargs)\n",
    "    save_predictions(preds, **kwargs)\n",
    "\n",
    "    return preds\n",
    "\n",
    "def main_submit(model,**kwargs):\n",
    "    make_and_save_predictions(model,**kwargs)\n",
    "    submit_kaggle(**kwargs)\n",
    "    return\n",
    "\n",
    "# Train model\n",
    "data_dict = main_load(whacky_mode=True)\n",
    "loss_vals,model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/411k [00:00<?, ?B/s]\n",
      "  2%|▏         | 8.00k/411k [00:00<00:07, 52.9kB/s]\n",
      " 76%|███████▌  | 312k/411k [00:00<00:00, 1.49MB/s] \n",
      "100%|██████████| 411k/411k [00:02<00:00, 194kB/s] \n",
      "\n",
      "Submission successful\n",
      "Successfully submitted to Playground Series - Season 3, Episode 1\n"
     ]
    }
   ],
   "source": [
    "main_submit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "675111725fca00cf5c47bbec148c05600f90b048f52a44ff5ac679806e49bdda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
